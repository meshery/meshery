{
  "capabilities": [
   {
    "description": "Initiate a performance test. Meshery will execute the load generation, collect metrics, and present the results.",
    "displayName": "Performance Test",
    "entityState": [
     "instance"
    ],
    "key": "",
    "kind": "action",
    "metadata": null,
    "schemaVersion": "capability.meshery.io/v1alpha1",
    "status": "enabled",
    "subType": "perf-test",
    "type": "operator",
    "version": "0.7.0"
   },
   {
    "description": "Configure the workload specific setting of a component",
    "displayName": "Workload Configuration",
    "entityState": [
     "declaration"
    ],
    "key": "",
    "kind": "mutate",
    "metadata": null,
    "schemaVersion": "capability.meshery.io/v1alpha1",
    "status": "enabled",
    "subType": "config",
    "type": "configuration",
    "version": "0.7.0"
   },
   {
    "description": "Configure Labels And Annotations for  the component ",
    "displayName": "Labels and Annotations Configuration",
    "entityState": [
     "declaration"
    ],
    "key": "",
    "kind": "mutate",
    "metadata": null,
    "schemaVersion": "capability.meshery.io/v1alpha1",
    "status": "enabled",
    "subType": "labels-and-annotations",
    "type": "configuration",
    "version": "0.7.0"
   },
   {
    "description": "View relationships for the component",
    "displayName": "Relationships",
    "entityState": [
     "declaration",
     "instance"
    ],
    "key": "",
    "kind": "view",
    "metadata": null,
    "schemaVersion": "capability.meshery.io/v1alpha1",
    "status": "enabled",
    "subType": "relationship",
    "type": "configuration",
    "version": "0.7.0"
   },
   {
    "description": "View Component Definition ",
    "displayName": "Json Schema",
    "entityState": [
     "declaration",
     "instance"
    ],
    "key": "",
    "kind": "view",
    "metadata": null,
    "schemaVersion": "capability.meshery.io/v1alpha1",
    "status": "enabled",
    "subType": "definition",
    "type": "configuration",
    "version": "0.7.0"
   },
   {
    "description": "Configure the visual styles for the component",
    "displayName": "Styling",
    "entityState": [
     "declaration"
    ],
    "key": "",
    "kind": "mutate",
    "metadata": null,
    "schemaVersion": "capability.meshery.io/v1alpha1",
    "status": "enabled",
    "subType": "",
    "type": "style",
    "version": "0.7.0"
   },
   {
    "description": "Change the shape of the component",
    "displayName": "Change Shape",
    "entityState": [
     "declaration"
    ],
    "key": "",
    "kind": "mutate",
    "metadata": null,
    "schemaVersion": "capability.meshery.io/v1alpha1",
    "status": "enabled",
    "subType": "shape",
    "type": "style",
    "version": "0.7.0"
   },
   {
    "description": "Drag and Drop a component into a parent component in graph view",
    "displayName": "Compound Drag And Drop",
    "entityState": [
     "declaration"
    ],
    "key": "",
    "kind": "interaction",
    "metadata": null,
    "schemaVersion": "capability.meshery.io/v1alpha1",
    "status": "enabled",
    "subType": "compoundDnd",
    "type": "graph",
    "version": "0.7.0"
   }
  ],
  "component": {
   "kind": "SGStream",
   "schema": "{\n \"properties\": {\n  \"spec\": {\n   \"description\": \"Specification of the desired behavior of a StackGres stream.\\n\\nA stream represent the process of performing a change data capture (CDC) operation on a data source that generates a stream of event containing information about the changes happening (or happened) to the database in real time (or from the beginning).\\n\\nThe stream allow to specify different types for the target of the CDC operation. See `SGStream.spec.target.type`.\\n\\nThe stream perform two distinct operation to generate data source changes for the target:\\n\\n* Snapshotting: allows to capture the content of the data source in a specific point in time and stream it as if they were changes, thus providing a stream of events as they were an aggregate from the beginning of the existence of the data source.\\n* Streaming: allows to capture the changes that are happening in real time in the data source and stream them as changes continuously.\\n\\nThe CDC is performed using [Debezium Engine](https://debezium.io/documentation/reference/stable/development/engine.html). SGStream extends functionality of Debezium by providing a [custom signaling channel](https://debezium.io/documentation/reference/stable/configuration/signalling.html#debezium-custom-signaling-channel) that allow to send signals by simply adding annotation to the SGStream resources.\\nTo send a signal simply create an annotation with the following formar:\\n\\n```\\nmetadata:\\n  annotations:\\n    debezium-signal.stackgres.io/\\u003csignal type\\u003e: \\u003csignal data\\u003e\\n```\\n\\nAlso, SGStream provide the following custom singals implementations:\\n  \\n  * `tombstone`: allow to stop completely Debezium streaming and the SGStream. This signal is useful to give an end to the streaming in a graceful way allowing for the removal of the logical slot created by Debezium.\\n  * `command`: allow to execute any SQL command on the target database. Only available then the target type is `SGCluster`.\\n\",\n   \"properties\": {\n    \"debeziumEngineProperties\": {\n     \"description\": \"See https://debezium.io/documentation/reference/stable/development/engine.html#engine-properties\\n Each property is converted from myPropertyName to my.property.name\\n\",\n     \"properties\": {\n      \"errorsMaxRetries\": {\n       \"description\": \"Default `-1`. The maximum number of retries on connection errors before failing (-1 = no limit, 0 = disabled, \\u003e 0 = num of retries).\\n\",\n       \"type\": \"integer\"\n      },\n      \"errorsRetryDelayInitialMs\": {\n       \"description\": \"Default `300`. Initial delay (in ms) for retries when encountering connection errors. This value will be doubled upon every retry but wonâ€™t exceed errorsRetryDelayMaxMs.\\n\",\n       \"type\": \"integer\"\n      },\n      \"errorsRetryDelayMaxMs\": {\n       \"description\": \"Default `10000`. Max delay (in ms) between retries when encountering conn\\n\",\n       \"type\": \"integer\"\n      },\n      \"offsetCommitPolicy\": {\n       \"description\": \"Default `io.debezium.engine.spi.OffsetCommitPolicy.PeriodicCommitOffsetPolicy`. The name of the Java class of the commit policy. It defines when offsets commit has to be triggered based on the number of events processed and the time elapsed since the last commit. This class must implement the interface OffsetCommitPolicy. The default is a periodic commity policy based upon time intervals.\\n\",\n       \"type\": \"string\"\n      },\n      \"offsetFlushIntervalMs\": {\n       \"description\": \"Default `60000`. Interval at which to try committing offsets. The default is 1 minute.\\n\",\n       \"type\": \"integer\"\n      },\n      \"offsetFlushTimeoutMs\": {\n       \"description\": \"Default `5000`. Maximum number of milliseconds to wait for records to flush and partition offset data to be committed to offset storage before cancelling the process and restoring the offset data to be committed in a future attempt. The default is 5 seconds.\\n\",\n       \"type\": \"integer\"\n      },\n      \"predicates\": {\n       \"additionalProperties\": {\n        \"additionalProperties\": {\n         \"type\": \"string\"\n        },\n        \"description\": \"The properties of this predicate.\",\n        \"type\": \"object\"\n       },\n       \"description\": \"Predicates can be applied to transformations to make the transformations optional.\\n\\nAn example of the configuration is:\\n\\n```\\npredicates:\\n  headerExists: # (1)\\n   type: \\\"org.apache.kafka.connect.transforms.predicates.HasHeaderKey\\\" # (2)\\n   name: \\\"header.name\\\" # (3)\\ntransforms:\\n  filter: # (4)\\n    type: \\\"io.debezium.embedded.ExampleFilterTransform\\\" # (5)\\n    predicate: \\\"headerExists\\\" # (6)\\n    negate: \\\"true\\\" # (7)\\n```\\n\\n1. One predicate is defined - headerExists\\n2. Implementation of the headerExists predicate is org.apache.kafka.connect.transforms.predicates.HasHeaderKey\\n3. The headerExists predicate has one configuration option - name\\n4. One transformation is defined - filter\\n5. Implementation of the filter transformation is io.debezium.embedded.ExampleFilterTransform\\n6. The filter transformation requires the predicate headerExists\\n7. The filter transformation expects the value of the predicate to be negated, making the predicate determine if the header does not exist\\n\",\n       \"type\": \"object\"\n      },\n      \"transforms\": {\n       \"additionalProperties\": {\n        \"additionalProperties\": {\n         \"type\": \"string\"\n        },\n        \"description\": \"The properties of this transformation.\",\n        \"type\": \"object\"\n       },\n       \"description\": \"Before the messages are delivered to the handler it is possible to run them through a pipeline of Kafka Connect Simple Message Transforms (SMT). Each SMT can pass the message unchanged, modify it or filter it out. The chain is configured using property transforms. The property contains a list of logical names of the transformations to be applied (the specified keys). Properties transforms.\\u003clogical_name\\u003e.type then defines the name of the implementation class for each transformation and transforms.\\u003clogical_name\\u003e.* configuration options that are passed to the transformation.\\n\\nAn example of the configuration is:\\n\\n```\\ntransforms: # (1)\\n  router:\\n    type: \\\"org.apache.kafka.connect.transforms.RegexRouter\\\" # (2)\\n    regex: \\\"(.*)\\\" # (3)\\n    replacement: \\\"trf$1\\\" # (3)\\n  filter:\\n    type: \\\"io.debezium.embedded.ExampleFilterTransform\\\" # (4)\\n```\\n\\n1. Two transformations are defined - filter and router\\n2. Implementation of the router transformation is org.apache.kafka.connect.transforms.RegexRouter\\n3. The router transformation has two configurations options -regex and replacement\\n4. Implementation of the filter transformation is io.debezium.embedded.ExampleFilterTransform\\n\",\n       \"type\": \"object\"\n      }\n     },\n     \"type\": \"object\"\n    },\n    \"maxRetries\": {\n     \"description\": \"The maximum number of retries the streaming operation is allowed to do after a failure.\\n\\nA value of `0` (zero) means no retries are made. A value of `-1` means retries are unlimited. Defaults to: `-1`.\\n\",\n     \"type\": \"integer\"\n    },\n    \"pods\": {\n     \"description\": \"The configuration for SGStream Pod\",\n     \"properties\": {\n      \"persistentVolume\": {\n       \"description\": \"Pod's persistent volume configuration.\\n\\n**Example:**\\n\\n```yaml\\napiVersion: stackgres.io/v1\\nkind: SGCluster\\nmetadata:\\n  name: stackgres\\nspec:\\n  pods:\\n    persistentVolume:\\n      size: '5Gi'\\n      storageClass: default\\n```\\n\",\n       \"properties\": {\n        \"size\": {\n         \"description\": \"Size of the PersistentVolume for stream Pod. This size is specified either in Mebibytes, Gibibytes or Tebibytes (multiples of 2^20, 2^30 or 2^40, respectively).\\n\",\n         \"pattern\": \"^[0-9]+(\\\\.[0-9]+)?(Mi|Gi|Ti)$\",\n         \"type\": \"string\"\n        },\n        \"storageClass\": {\n         \"description\": \"Name of an existing StorageClass in the Kubernetes cluster, used to create the PersistentVolume for stream.\\n\",\n         \"type\": \"string\"\n        }\n       },\n       \"required\": [\n        \"size\"\n       ],\n       \"type\": \"object\"\n      },\n      \"resources\": {\n       \"description\": \"The resources assigned to the stream container.\\n\\nSee https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\\n\",\n       \"properties\": {\n        \"claims\": {\n         \"description\": \"Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.\\n\\nThis is an alpha field and requires enabling the DynamicResourceAllocation feature gate.\\n\\nThis field is immutable. It can only be set for containers.\",\n         \"items\": {\n          \"description\": \"ResourceClaim references one entry in PodSpec.ResourceClaims.\",\n          \"properties\": {\n           \"name\": {\n            \"description\": \"Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container.\",\n            \"type\": \"string\"\n           }\n          },\n          \"required\": [\n           \"name\"\n          ],\n          \"type\": \"object\"\n         },\n         \"type\": \"array\"\n        },\n        \"limits\": {\n         \"additionalProperties\": {\n          \"description\": \"Quantity is a fixed-point representation of a number. It provides convenient marshaling/unmarshaling in JSON and YAML, in addition to String() and AsInt64() accessors.\\n\\nThe serialization format is:\\n\\n``` \\u003cquantity\\u003e        ::= \\u003csignedNumber\\u003e\\u003csuffix\\u003e\\n\\n\\t(Note that \\u003csuffix\\u003e may be empty, from the \\\"\\\" case in \\u003cdecimalSI\\u003e.)\\n\\n\\u003cdigit\\u003e           ::= 0 | 1 | ... | 9 \\u003cdigits\\u003e          ::= \\u003cdigit\\u003e | \\u003cdigit\\u003e\\u003cdigits\\u003e \\u003cnumber\\u003e          ::= \\u003cdigits\\u003e | \\u003cdigits\\u003e.\\u003cdigits\\u003e | \\u003cdigits\\u003e. | .\\u003cdigits\\u003e \\u003csign\\u003e            ::= \\\"+\\\" | \\\"-\\\" \\u003csignedNumber\\u003e    ::= \\u003cnumber\\u003e | \\u003csign\\u003e\\u003cnumber\\u003e \\u003csuffix\\u003e          ::= \\u003cbinarySI\\u003e | \\u003cdecimalExponent\\u003e | \\u003cdecimalSI\\u003e \\u003cbinarySI\\u003e        ::= Ki | Mi | Gi | Ti | Pi | Ei\\n\\n\\t(International System of units; See: http://physics.nist.gov/cuu/Units/binary.html)\\n\\n\\u003cdecimalSI\\u003e       ::= m | \\\"\\\" | k | M | G | T | P | E\\n\\n\\t(Note that 1024 = 1Ki but 1000 = 1k; I didn't choose the capitalization.)\\n\\n\\u003cdecimalExponent\\u003e ::= \\\"e\\\" \\u003csignedNumber\\u003e | \\\"E\\\" \\u003csignedNumber\\u003e ```\\n\\nNo matter which of the three exponent forms is used, no quantity may represent a number greater than 2^63-1 in magnitude, nor may it have more than 3 decimal places. Numbers larger or more precise will be capped or rounded up. (E.g.: 0.1m will rounded up to 1m.) This may be extended in the future if we require larger or smaller quantities.\\n\\nWhen a Quantity is parsed from a string, it will remember the type of suffix it had, and will use the same type again when it is serialized.\\n\\nBefore serializing, Quantity will be put in \\\"canonical form\\\". This means that Exponent/suffix will be adjusted up or down (with a corresponding increase or decrease in Mantissa) such that:\\n\\n- No precision is lost - No fractional digits will be emitted - The exponent (or suffix) is as large as possible.\\n\\nThe sign will be omitted unless the number is negative.\\n\\nExamples:\\n\\n- 1.5 will be serialized as \\\"1500m\\\" - 1.5Gi will be serialized as \\\"1536Mi\\\"\\n\\nNote that the quantity will NEVER be internally represented by a floating point number. That is the whole point of this exercise.\\n\\nNon-canonical values will still parse as long as they are well formed, but will be re-emitted in their canonical form. (So always use canonical form, or don't diff.)\\n\\nThis format is intended to make it difficult to use these numbers without writing some sort of special handling code in the hopes that that will cause implementors to also use a fixed point implementation.\",\n          \"type\": \"string\"\n         },\n         \"description\": \"Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\",\n         \"type\": \"object\"\n        },\n        \"requests\": {\n         \"additionalProperties\": {\n          \"description\": \"Quantity is a fixed-point representation of a number. It provides convenient marshaling/unmarshaling in JSON and YAML, in addition to String() and AsInt64() accessors.\\n\\nThe serialization format is:\\n\\n``` \\u003cquantity\\u003e        ::= \\u003csignedNumber\\u003e\\u003csuffix\\u003e\\n\\n\\t(Note that \\u003csuffix\\u003e may be empty, from the \\\"\\\" case in \\u003cdecimalSI\\u003e.)\\n\\n\\u003cdigit\\u003e           ::= 0 | 1 | ... | 9 \\u003cdigits\\u003e          ::= \\u003cdigit\\u003e | \\u003cdigit\\u003e\\u003cdigits\\u003e \\u003cnumber\\u003e          ::= \\u003cdigits\\u003e | \\u003cdigits\\u003e.\\u003cdigits\\u003e | \\u003cdigits\\u003e. | .\\u003cdigits\\u003e \\u003csign\\u003e            ::= \\\"+\\\" | \\\"-\\\" \\u003csignedNumber\\u003e    ::= \\u003cnumber\\u003e | \\u003csign\\u003e\\u003cnumber\\u003e \\u003csuffix\\u003e          ::= \\u003cbinarySI\\u003e | \\u003cdecimalExponent\\u003e | \\u003cdecimalSI\\u003e \\u003cbinarySI\\u003e        ::= Ki | Mi | Gi | Ti | Pi | Ei\\n\\n\\t(International System of units; See: http://physics.nist.gov/cuu/Units/binary.html)\\n\\n\\u003cdecimalSI\\u003e       ::= m | \\\"\\\" | k | M | G | T | P | E\\n\\n\\t(Note that 1024 = 1Ki but 1000 = 1k; I didn't choose the capitalization.)\\n\\n\\u003cdecimalExponent\\u003e ::= \\\"e\\\" \\u003csignedNumber\\u003e | \\\"E\\\" \\u003csignedNumber\\u003e ```\\n\\nNo matter which of the three exponent forms is used, no quantity may represent a number greater than 2^63-1 in magnitude, nor may it have more than 3 decimal places. Numbers larger or more precise will be capped or rounded up. (E.g.: 0.1m will rounded up to 1m.) This may be extended in the future if we require larger or smaller quantities.\\n\\nWhen a Quantity is parsed from a string, it will remember the type of suffix it had, and will use the same type again when it is serialized.\\n\\nBefore serializing, Quantity will be put in \\\"canonical form\\\". This means that Exponent/suffix will be adjusted up or down (with a corresponding increase or decrease in Mantissa) such that:\\n\\n- No precision is lost - No fractional digits will be emitted - The exponent (or suffix) is as large as possible.\\n\\nThe sign will be omitted unless the number is negative.\\n\\nExamples:\\n\\n- 1.5 will be serialized as \\\"1500m\\\" - 1.5Gi will be serialized as \\\"1536Mi\\\"\\n\\nNote that the quantity will NEVER be internally represented by a floating point number. That is the whole point of this exercise.\\n\\nNon-canonical values will still parse as long as they are well formed, but will be re-emitted in their canonical form. (So always use canonical form, or don't diff.)\\n\\nThis format is intended to make it difficult to use these numbers without writing some sort of special handling code in the hopes that that will cause implementors to also use a fixed point implementation.\",\n          \"type\": \"string\"\n         },\n         \"description\": \"Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\",\n         \"type\": \"object\"\n        }\n       },\n       \"type\": \"object\"\n      },\n      \"scheduling\": {\n       \"description\": \"Pod custom scheduling, affinity and topology spread constratins configuration.\\n\",\n       \"properties\": {\n        \"nodeAffinity\": {\n         \"description\": \"Node affinity is a group of node affinity scheduling rules.\\n\\nSee https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#nodeaffinity-v1-core\",\n         \"properties\": {\n          \"preferredDuringSchedulingIgnoredDuringExecution\": {\n           \"description\": \"The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \\\"weight\\\" to the sum if the node matches the corresponding matchExpressions; the node(s) with the highest sum are the most preferred.\",\n           \"items\": {\n            \"description\": \"An empty preferred scheduling term matches all objects with implicit weight 0 (i.e. it's a no-op). A null preferred scheduling term matches no objects (i.e. is also a no-op).\",\n            \"properties\": {\n             \"preference\": {\n              \"description\": \"A null or empty node selector term matches no objects. The requirements of them are ANDed. The TopologySelectorTerm type implements a subset of the NodeSelectorTerm.\",\n              \"properties\": {\n               \"matchExpressions\": {\n                \"description\": \"A list of node selector requirements by node's labels.\",\n                \"items\": {\n                 \"description\": \"A node selector requirement is a selector that contains values, a key, and an operator that relates the key and values.\",\n                 \"properties\": {\n                  \"key\": {\n                   \"description\": \"The label key that the selector applies to.\",\n                   \"type\": \"string\"\n                  },\n                  \"operator\": {\n                   \"description\": \"Represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.\",\n                   \"type\": \"string\"\n                  },\n                  \"values\": {\n                   \"description\": \"An array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. If the operator is Gt or Lt, the values array must have a single element, which will be interpreted as an integer. This array is replaced during a strategic merge patch.\",\n                   \"items\": {\n                    \"type\": \"string\"\n                   },\n                   \"type\": \"array\"\n                  }\n                 },\n                 \"required\": [\n                  \"key\",\n                  \"operator\"\n                 ],\n                 \"type\": \"object\"\n                },\n                \"type\": \"array\"\n               },\n               \"matchFields\": {\n                \"description\": \"A list of node selector requirements by node's fields.\",\n                \"items\": {\n                 \"description\": \"A node selector requirement is a selector that contains values, a key, and an operator that relates the key and values.\",\n                 \"properties\": {\n                  \"key\": {\n                   \"description\": \"The label key that the selector applies to.\",\n                   \"type\": \"string\"\n                  },\n                  \"operator\": {\n                   \"description\": \"Represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.\",\n                   \"type\": \"string\"\n                  },\n                  \"values\": {\n                   \"description\": \"An array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. If the operator is Gt or Lt, the values array must have a single element, which will be interpreted as an integer. This array is replaced during a strategic merge patch.\",\n                   \"items\": {\n                    \"type\": \"string\"\n                   },\n                   \"type\": \"array\"\n                  }\n                 },\n                 \"required\": [\n                  \"key\",\n                  \"operator\"\n                 ],\n                 \"type\": \"object\"\n                },\n                \"type\": \"array\"\n               }\n              },\n              \"type\": \"object\"\n             },\n             \"weight\": {\n              \"description\": \"Weight associated with matching the corresponding nodeSelectorTerm, in the range 1-100.\",\n              \"format\": \"int32\",\n              \"type\": \"integer\"\n             }\n            },\n            \"required\": [\n             \"weight\",\n             \"preference\"\n            ],\n            \"type\": \"object\"\n           },\n           \"type\": \"array\"\n          },\n          \"requiredDuringSchedulingIgnoredDuringExecution\": {\n           \"description\": \"A node selector represents the union of the results of one or more label queries over a set of nodes; that is, it represents the OR of the selectors represented by the node selector terms.\",\n           \"properties\": {\n            \"nodeSelectorTerms\": {\n             \"description\": \"Required. A list of node selector terms. The terms are ORed.\",\n             \"items\": {\n              \"description\": \"A null or empty node selector term matches no objects. The requirements of them are ANDed. The TopologySelectorTerm type implements a subset of the NodeSelectorTerm.\",\n              \"properties\": {\n               \"matchExpressions\": {\n                \"description\": \"A list of node selector requirements by node's labels.\",\n                \"items\": {\n                 \"description\": \"A node selector requirement is a selector that contains values, a key, and an operator that relates the key and values.\",\n                 \"properties\": {\n                  \"key\": {\n                   \"description\": \"The label key that the selector applies to.\",\n                   \"type\": \"string\"\n                  },\n                  \"operator\": {\n                   \"description\": \"Represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.\",\n                   \"type\": \"string\"\n                  },\n                  \"values\": {\n                   \"description\": \"An array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. If the operator is Gt or Lt, the values array must have a single element, which will be interpreted as an integer. This array is replaced during a strategic merge patch.\",\n                   \"items\": {\n                    \"type\": \"string\"\n                   },\n                   \"type\": \"array\"\n                  }\n                 },\n                 \"required\": [\n                  \"key\",\n                  \"operator\"\n                 ],\n                 \"type\": \"object\"\n                },\n                \"type\": \"array\"\n               },\n               \"matchFields\": {\n                \"description\": \"A list of node selector requirements by node's fields.\",\n                \"items\": {\n                 \"description\": \"A node selector requirement is a selector that contains values, a key, and an operator that relates the key and values.\",\n                 \"properties\": {\n                  \"key\": {\n                   \"description\": \"The label key that the selector applies to.\",\n                   \"type\": \"string\"\n                  },\n                  \"operator\": {\n                   \"description\": \"Represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt.\",\n                   \"type\": \"string\"\n                  },\n                  \"values\": {\n                   \"description\": \"An array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. If the operator is Gt or Lt, the values array must have a single element, which will be interpreted as an integer. This array is replaced during a strategic merge patch.\",\n                   \"items\": {\n                    \"type\": \"string\"\n                   },\n                   \"type\": \"array\"\n                  }\n                 },\n                 \"required\": [\n                  \"key\",\n                  \"operator\"\n                 ],\n                 \"type\": \"object\"\n                },\n                \"type\": \"array\"\n               }\n              },\n              \"type\": \"object\"\n             },\n             \"type\": \"array\"\n            }\n           },\n           \"required\": [\n            \"nodeSelectorTerms\"\n           ],\n           \"type\": \"object\"\n          }\n         },\n         \"type\": \"object\"\n        },\n        \"nodeSelector\": {\n         \"additionalProperties\": {\n          \"type\": \"string\"\n         },\n         \"description\": \"NodeSelector is a selector which must be true for the pod to fit on a node. Selector which must match a node's labels for the pod to be scheduled on that node. More info: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\\n\",\n         \"type\": \"object\"\n        },\n        \"podAffinity\": {\n         \"description\": \"Pod affinity is a group of inter pod affinity scheduling rules.\\n\\nSee https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#podaffinity-v1-core\",\n         \"properties\": {\n          \"preferredDuringSchedulingIgnoredDuringExecution\": {\n           \"description\": \"The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \\\"weight\\\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most preferred.\",\n           \"items\": {\n            \"description\": \"The weights of all of the matched WeightedPodAffinityTerm fields are added per-node to find the most preferred node(s)\",\n            \"properties\": {\n             \"podAffinityTerm\": {\n              \"description\": \"Defines a set of pods (namely those matching the labelSelector relative to the given namespace(s)) that this pod should be co-located (affinity) or not co-located (anti-affinity) with, where co-located is defined as running on a node whose value of the label with key \\u003ctopologyKey\\u003e matches that of any node on which a pod of the set of pods is running\",\n              \"properties\": {\n               \"labelSelector\": {\n                \"description\": \"A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects.\",\n                \"properties\": {\n                 \"matchExpressions\": {\n                  \"description\": \"matchExpressions is a list of label selector requirements. The requirements are ANDed.\",\n                  \"items\": {\n                   \"description\": \"A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.\",\n                   \"properties\": {\n                    \"key\": {\n                     \"description\": \"key is the label key that the selector applies to.\",\n                     \"type\": \"string\"\n                    },\n                    \"operator\": {\n                     \"description\": \"operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.\",\n                     \"type\": \"string\"\n                    },\n                    \"values\": {\n                     \"description\": \"values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.\",\n                     \"items\": {\n                      \"type\": \"string\"\n                     },\n                     \"type\": \"array\"\n                    }\n                   },\n                   \"required\": [\n                    \"key\",\n                    \"operator\"\n                   ],\n                   \"type\": \"object\"\n                  },\n                  \"type\": \"array\"\n                 },\n                 \"matchLabels\": {\n                  \"additionalProperties\": {\n                   \"type\": \"string\"\n                  },\n                  \"description\": \"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \\\"key\\\", the operator is \\\"In\\\", and the values array contains only \\\"value\\\". The requirements are ANDed.\",\n                  \"type\": \"object\"\n                 }\n                },\n                \"type\": \"object\"\n               },\n               \"matchLabelKeys\": {\n                \"description\": \"MatchLabelKeys is a set of pod label keys to select which pods will be taken into consideration. The keys are used to lookup values from the incoming pod labels, those key-value labels are merged with `LabelSelector` as `key in (value)` to select the group of existing pods which pods will be taken into consideration for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming pod labels will be ignored. The default value is empty. The same key is forbidden to exist in both MatchLabelKeys and LabelSelector. Also, MatchLabelKeys cannot be set when LabelSelector isn't set. This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.\",\n                \"items\": {\n                 \"type\": \"string\"\n                },\n                \"type\": \"array\"\n               },\n               \"mismatchLabelKeys\": {\n                \"description\": \"MismatchLabelKeys is a set of pod label keys to select which pods will be taken into consideration. The keys are used to lookup values from the incoming pod labels, those key-value labels are merged with `LabelSelector` as `key notin (value)` to select the group of existing pods which pods will be taken into consideration for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming pod labels will be ignored. The default value is empty. The same key is forbidden to exist in both MismatchLabelKeys and LabelSelector. Also, MismatchLabelKeys cannot be set when LabelSelector isn't set. This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.\",\n                \"items\": {\n                 \"type\": \"string\"\n                },\n                \"type\": \"array\"\n               },\n               \"namespaceSelector\": {\n                \"description\": \"A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects.\",\n                \"properties\": {\n                 \"matchExpressions\": {\n                  \"description\": \"matchExpressions is a list of label selector requirements. The requirements are ANDed.\",\n                  \"items\": {\n                   \"description\": \"A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.\",\n                   \"properties\": {\n                    \"key\": {\n                     \"description\": \"key is the label key that the selector applies to.\",\n                     \"type\": \"string\"\n                    },\n                    \"operator\": {\n                     \"description\": \"operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.\",\n                     \"type\": \"string\"\n                    },\n                    \"values\": {\n                     \"description\": \"values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.\",\n                     \"items\": {\n                      \"type\": \"string\"\n                     },\n                     \"type\": \"array\"\n                    }\n                   },\n                   \"required\": [\n                    \"key\",\n                    \"operator\"\n                   ],\n                   \"type\": \"object\"\n                  },\n                  \"type\": \"array\"\n                 },\n                 \"matchLabels\": {\n                  \"additionalProperties\": {\n                   \"type\": \"string\"\n                  },\n                  \"description\": \"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \\\"key\\\", the operator is \\\"In\\\", and the values array contains only \\\"value\\\". The requirements are ANDed.\",\n                  \"type\": \"object\"\n                 }\n                },\n                \"type\": \"object\"\n               },\n               \"namespaces\": {\n                \"description\": \"namespaces specifies a static list of namespace names that the term applies to. The term is applied to the union of the namespaces listed in this field and the ones selected by namespaceSelector. null or empty namespaces list and null namespaceSelector means \\\"this pod's namespace\\\".\",\n                \"items\": {\n                 \"type\": \"string\"\n                },\n                \"type\": \"array\"\n               },\n               \"topologyKey\": {\n                \"description\": \"This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching the labelSelector in the specified namespaces, where co-located is defined as running on a node whose value of the label with key topologyKey matches that of any node on which any of the selected pods is running. Empty topologyKey is not allowed.\",\n                \"type\": \"string\"\n               }\n              },\n              \"required\": [\n               \"topologyKey\"\n              ],\n              \"type\": \"object\"\n             },\n             \"weight\": {\n              \"description\": \"weight associated with matching the corresponding podAffinityTerm, in the range 1-100.\",\n              \"format\": \"int32\",\n              \"type\": \"integer\"\n             }\n            },\n            \"required\": [\n             \"weight\",\n             \"podAffinityTerm\"\n            ],\n            \"type\": \"object\"\n           },\n           \"type\": \"array\"\n          },\n          \"requiredDuringSchedulingIgnoredDuringExecution\": {\n           \"description\": \"If the affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied.\",\n           \"items\": {\n            \"description\": \"Defines a set of pods (namely those matching the labelSelector relative to the given namespace(s)) that this pod should be co-located (affinity) or not co-located (anti-affinity) with, where co-located is defined as running on a node whose value of the label with key \\u003ctopologyKey\\u003e matches that of any node on which a pod of the set of pods is running\",\n            \"properties\": {\n             \"labelSelector\": {\n              \"description\": \"A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects.\",\n              \"properties\": {\n               \"matchExpressions\": {\n                \"description\": \"matchExpressions is a list of label selector requirements. The requirements are ANDed.\",\n                \"items\": {\n                 \"description\": \"A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.\",\n                 \"properties\": {\n                  \"key\": {\n                   \"description\": \"key is the label key that the selector applies to.\",\n                   \"type\": \"string\"\n                  },\n                  \"operator\": {\n                   \"description\": \"operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.\",\n                   \"type\": \"string\"\n                  },\n                  \"values\": {\n                   \"description\": \"values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.\",\n                   \"items\": {\n                    \"type\": \"string\"\n                   },\n                   \"type\": \"array\"\n                  }\n                 },\n                 \"required\": [\n                  \"key\",\n                  \"operator\"\n                 ],\n                 \"type\": \"object\"\n                },\n                \"type\": \"array\"\n               },\n               \"matchLabels\": {\n                \"additionalProperties\": {\n                 \"type\": \"string\"\n                },\n                \"description\": \"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \\\"key\\\", the operator is \\\"In\\\", and the values array contains only \\\"value\\\". The requirements are ANDed.\",\n                \"type\": \"object\"\n               }\n              },\n              \"type\": \"object\"\n             },\n             \"matchLabelKeys\": {\n              \"description\": \"MatchLabelKeys is a set of pod label keys to select which pods will be taken into consideration. The keys are used to lookup values from the incoming pod labels, those key-value labels are merged with `LabelSelector` as `key in (value)` to select the group of existing pods which pods will be taken into consideration for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming pod labels will be ignored. The default value is empty. The same key is forbidden to exist in both MatchLabelKeys and LabelSelector. Also, MatchLabelKeys cannot be set when LabelSelector isn't set. This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.\",\n              \"items\": {\n               \"type\": \"string\"\n              },\n              \"type\": \"array\"\n             },\n             \"mismatchLabelKeys\": {\n              \"description\": \"MismatchLabelKeys is a set of pod label keys to select which pods will be taken into consideration. The keys are used to lookup values from the incoming pod labels, those key-value labels are merged with `LabelSelector` as `key notin (value)` to select the group of existing pods which pods will be taken into consideration for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming pod labels will be ignored. The default value is empty. The same key is forbidden to exist in both MismatchLabelKeys and LabelSelector. Also, MismatchLabelKeys cannot be set when LabelSelector isn't set. This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.\",\n              \"items\": {\n               \"type\": \"string\"\n              },\n              \"type\": \"array\"\n             },\n             \"namespaceSelector\": {\n              \"description\": \"A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects.\",\n              \"properties\": {\n               \"matchExpressions\": {\n                \"description\": \"matchExpressions is a list of label selector requirements. The requirements are ANDed.\",\n                \"items\": {\n                 \"description\": \"A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.\",\n                 \"properties\": {\n                  \"key\": {\n                   \"description\": \"key is the label key that the selector applies to.\",\n                   \"type\": \"string\"\n                  },\n                  \"operator\": {\n                   \"description\": \"operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.\",\n                   \"type\": \"string\"\n                  },\n                  \"values\": {\n                   \"description\": \"values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.\",\n                   \"items\": {\n                    \"type\": \"string\"\n                   },\n                   \"type\": \"array\"\n                  }\n                 },\n                 \"required\": [\n                  \"key\",\n                  \"operator\"\n                 ],\n                 \"type\": \"object\"\n                },\n                \"type\": \"array\"\n               },\n               \"matchLabels\": {\n                \"additionalProperties\": {\n                 \"type\": \"string\"\n                },\n                \"description\": \"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \\\"key\\\", the operator is \\\"In\\\", and the values array contains only \\\"value\\\". The requirements are ANDed.\",\n                \"type\": \"object\"\n               }\n              },\n              \"type\": \"object\"\n             },\n             \"namespaces\": {\n              \"description\": \"namespaces specifies a static list of namespace names that the term applies to. The term is applied to the union of the namespaces listed in this field and the ones selected by namespaceSelector. null or empty namespaces list and null namespaceSelector means \\\"this pod's namespace\\\".\",\n              \"items\": {\n               \"type\": \"string\"\n              },\n              \"type\": \"array\"\n             },\n             \"topologyKey\": {\n              \"description\": \"This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching the labelSelector in the specified namespaces, where co-located is defined as running on a node whose value of the label with key topologyKey matches that of any node on which any of the selected pods is running. Empty topologyKey is not allowed.\",\n              \"type\": \"string\"\n             }\n            },\n            \"required\": [\n             \"topologyKey\"\n            ],\n            \"type\": \"object\"\n           },\n           \"type\": \"array\"\n          }\n         },\n         \"type\": \"object\"\n        },\n        \"podAntiAffinity\": {\n         \"description\": \"Pod anti affinity is a group of inter pod anti affinity scheduling rules.\\n\\nSee https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#podantiaffinity-v1-core\",\n         \"properties\": {\n          \"preferredDuringSchedulingIgnoredDuringExecution\": {\n           \"description\": \"The scheduler will prefer to schedule pods to nodes that satisfy the anti-affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling anti-affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \\\"weight\\\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most preferred.\",\n           \"items\": {\n            \"description\": \"The weights of all of the matched WeightedPodAffinityTerm fields are added per-node to find the most preferred node(s)\",\n            \"properties\": {\n             \"podAffinityTerm\": {\n              \"description\": \"Defines a set of pods (namely those matching the labelSelector relative to the given namespace(s)) that this pod should be co-located (affinity) or not co-located (anti-affinity) with, where co-located is defined as running on a node whose value of the label with key \\u003ctopologyKey\\u003e matches that of any node on which a pod of the set of pods is running\",\n              \"properties\": {\n               \"labelSelector\": {\n                \"description\": \"A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects.\",\n                \"properties\": {\n                 \"matchExpressions\": {\n                  \"description\": \"matchExpressions is a list of label selector requirements. The requirements are ANDed.\",\n                  \"items\": {\n                   \"description\": \"A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.\",\n                   \"properties\": {\n                    \"key\": {\n                     \"description\": \"key is the label key that the selector applies to.\",\n                     \"type\": \"string\"\n                    },\n                    \"operator\": {\n                     \"description\": \"operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.\",\n                     \"type\": \"string\"\n                    },\n                    \"values\": {\n                     \"description\": \"values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.\",\n                     \"items\": {\n                      \"type\": \"string\"\n                     },\n                     \"type\": \"array\"\n                    }\n                   },\n                   \"required\": [\n                    \"key\",\n                    \"operator\"\n                   ],\n                   \"type\": \"object\"\n                  },\n                  \"type\": \"array\"\n                 },\n                 \"matchLabels\": {\n                  \"additionalProperties\": {\n                   \"type\": \"string\"\n                  },\n                  \"description\": \"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \\\"key\\\", the operator is \\\"In\\\", and the values array contains only \\\"value\\\". The requirements are ANDed.\",\n                  \"type\": \"object\"\n                 }\n                },\n                \"type\": \"object\"\n               },\n               \"matchLabelKeys\": {\n                \"description\": \"MatchLabelKeys is a set of pod label keys to select which pods will be taken into consideration. The keys are used to lookup values from the incoming pod labels, those key-value labels are merged with `LabelSelector` as `key in (value)` to select the group of existing pods which pods will be taken into consideration for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming pod labels will be ignored. The default value is empty. The same key is forbidden to exist in both MatchLabelKeys and LabelSelector. Also, MatchLabelKeys cannot be set when LabelSelector isn't set. This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.\",\n                \"items\": {\n                 \"type\": \"string\"\n                },\n                \"type\": \"array\"\n               },\n               \"mismatchLabelKeys\": {\n                \"description\": \"MismatchLabelKeys is a set of pod label keys to select which pods will be taken into consideration. The keys are used to lookup values from the incoming pod labels, those key-value labels are merged with `LabelSelector` as `key notin (value)` to select the group of existing pods which pods will be taken into consideration for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming pod labels will be ignored. The default value is empty. The same key is forbidden to exist in both MismatchLabelKeys and LabelSelector. Also, MismatchLabelKeys cannot be set when LabelSelector isn't set. This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.\",\n                \"items\": {\n                 \"type\": \"string\"\n                },\n                \"type\": \"array\"\n               },\n               \"namespaceSelector\": {\n                \"description\": \"A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects.\",\n                \"properties\": {\n                 \"matchExpressions\": {\n                  \"description\": \"matchExpressions is a list of label selector requirements. The requirements are ANDed.\",\n                  \"items\": {\n                   \"description\": \"A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.\",\n                   \"properties\": {\n                    \"key\": {\n                     \"description\": \"key is the label key that the selector applies to.\",\n                     \"type\": \"string\"\n                    },\n                    \"operator\": {\n                     \"description\": \"operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.\",\n                     \"type\": \"string\"\n                    },\n                    \"values\": {\n                     \"description\": \"values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.\",\n                     \"items\": {\n                      \"type\": \"string\"\n                     },\n                     \"type\": \"array\"\n                    }\n                   },\n                   \"required\": [\n                    \"key\",\n                    \"operator\"\n                   ],\n                   \"type\": \"object\"\n                  },\n                  \"type\": \"array\"\n                 },\n                 \"matchLabels\": {\n                  \"additionalProperties\": {\n                   \"type\": \"string\"\n                  },\n                  \"description\": \"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \\\"key\\\", the operator is \\\"In\\\", and the values array contains only \\\"value\\\". The requirements are ANDed.\",\n                  \"type\": \"object\"\n                 }\n                },\n                \"type\": \"object\"\n               },\n               \"namespaces\": {\n                \"description\": \"namespaces specifies a static list of namespace names that the term applies to. The term is applied to the union of the namespaces listed in this field and the ones selected by namespaceSelector. null or empty namespaces list and null namespaceSelector means \\\"this pod's namespace\\\".\",\n                \"items\": {\n                 \"type\": \"string\"\n                },\n                \"type\": \"array\"\n               },\n               \"topologyKey\": {\n                \"description\": \"This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching the labelSelector in the specified namespaces, where co-located is defined as running on a node whose value of the label with key topologyKey matches that of any node on which any of the selected pods is running. Empty topologyKey is not allowed.\",\n                \"type\": \"string\"\n               }\n              },\n              \"required\": [\n               \"topologyKey\"\n              ],\n              \"type\": \"object\"\n             },\n             \"weight\": {\n              \"description\": \"weight associated with matching the corresponding podAffinityTerm, in the range 1-100.\",\n              \"format\": \"int32\",\n              \"type\": \"integer\"\n             }\n            },\n            \"required\": [\n             \"weight\",\n             \"podAffinityTerm\"\n            ],\n            \"type\": \"object\"\n           },\n           \"type\": \"array\"\n          },\n          \"requiredDuringSchedulingIgnoredDuringExecution\": {\n           \"description\": \"If the anti-affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the anti-affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied.\",\n           \"items\": {\n            \"description\": \"Defines a set of pods (namely those matching the labelSelector relative to the given namespace(s)) that this pod should be co-located (affinity) or not co-located (anti-affinity) with, where co-located is defined as running on a node whose value of the label with key \\u003ctopologyKey\\u003e matches that of any node on which a pod of the set of pods is running\",\n            \"properties\": {\n             \"labelSelector\": {\n              \"description\": \"A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects.\",\n              \"properties\": {\n               \"matchExpressions\": {\n                \"description\": \"matchExpressions is a list of label selector requirements. The requirements are ANDed.\",\n                \"items\": {\n                 \"description\": \"A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.\",\n                 \"properties\": {\n                  \"key\": {\n                   \"description\": \"key is the label key that the selector applies to.\",\n                   \"type\": \"string\"\n                  },\n                  \"operator\": {\n                   \"description\": \"operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.\",\n                   \"type\": \"string\"\n                  },\n                  \"values\": {\n                   \"description\": \"values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.\",\n                   \"items\": {\n                    \"type\": \"string\"\n                   },\n                   \"type\": \"array\"\n                  }\n                 },\n                 \"required\": [\n                  \"key\",\n                  \"operator\"\n                 ],\n                 \"type\": \"object\"\n                },\n                \"type\": \"array\"\n               },\n               \"matchLabels\": {\n                \"additionalProperties\": {\n                 \"type\": \"string\"\n                },\n                \"description\": \"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \\\"key\\\", the operator is \\\"In\\\", and the values array contains only \\\"value\\\". The requirements are ANDed.\",\n                \"type\": \"object\"\n               }\n              },\n              \"type\": \"object\"\n             },\n             \"matchLabelKeys\": {\n              \"description\": \"MatchLabelKeys is a set of pod label keys to select which pods will be taken into consideration. The keys are used to lookup values from the incoming pod labels, those key-value labels are merged with `LabelSelector` as `key in (value)` to select the group of existing pods which pods will be taken into consideration for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming pod labels will be ignored. The default value is empty. The same key is forbidden to exist in both MatchLabelKeys and LabelSelector. Also, MatchLabelKeys cannot be set when LabelSelector isn't set. This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.\",\n              \"items\": {\n               \"type\": \"string\"\n              },\n              \"type\": \"array\"\n             },\n             \"mismatchLabelKeys\": {\n              \"description\": \"MismatchLabelKeys is a set of pod label keys to select which pods will be taken into consideration. The keys are used to lookup values from the incoming pod labels, those key-value labels are merged with `LabelSelector` as `key notin (value)` to select the group of existing pods which pods will be taken into consideration for the incoming pod's pod (anti) affinity. Keys that don't exist in the incoming pod labels will be ignored. The default value is empty. The same key is forbidden to exist in both MismatchLabelKeys and LabelSelector. Also, MismatchLabelKeys cannot be set when LabelSelector isn't set. This is an alpha field and requires enabling MatchLabelKeysInPodAffinity feature gate.\",\n              \"items\": {\n               \"type\": \"string\"\n              },\n              \"type\": \"array\"\n             },\n             \"namespaceSelector\": {\n              \"description\": \"A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects.\",\n              \"properties\": {\n               \"matchExpressions\": {\n                \"description\": \"matchExpressions is a list of label selector requirements. The requirements are ANDed.\",\n                \"items\": {\n                 \"description\": \"A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.\",\n                 \"properties\": {\n                  \"key\": {\n                   \"description\": \"key is the label key that the selector applies to.\",\n                   \"type\": \"string\"\n                  },\n                  \"operator\": {\n                   \"description\": \"operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.\",\n                   \"type\": \"string\"\n                  },\n                  \"values\": {\n                   \"description\": \"values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.\",\n                   \"items\": {\n                    \"type\": \"string\"\n                   },\n                   \"type\": \"array\"\n                  }\n                 },\n                 \"required\": [\n                  \"key\",\n                  \"operator\"\n                 ],\n                 \"type\": \"object\"\n                },\n                \"type\": \"array\"\n               },\n               \"matchLabels\": {\n                \"additionalProperties\": {\n                 \"type\": \"string\"\n                },\n                \"description\": \"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \\\"key\\\", the operator is \\\"In\\\", and the values array contains only \\\"value\\\". The requirements are ANDed.\",\n                \"type\": \"object\"\n               }\n              },\n              \"type\": \"object\"\n             },\n             \"namespaces\": {\n              \"description\": \"namespaces specifies a static list of namespace names that the term applies to. The term is applied to the union of the namespaces listed in this field and the ones selected by namespaceSelector. null or empty namespaces list and null namespaceSelector means \\\"this pod's namespace\\\".\",\n              \"items\": {\n               \"type\": \"string\"\n              },\n              \"type\": \"array\"\n             },\n             \"topologyKey\": {\n              \"description\": \"This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching the labelSelector in the specified namespaces, where co-located is defined as running on a node whose value of the label with key topologyKey matches that of any node on which any of the selected pods is running. Empty topologyKey is not allowed.\",\n              \"type\": \"string\"\n             }\n            },\n            \"required\": [\n             \"topologyKey\"\n            ],\n            \"type\": \"object\"\n           },\n           \"type\": \"array\"\n          }\n         },\n         \"type\": \"object\"\n        },\n        \"priorityClassName\": {\n         \"description\": \"If specified, indicates the pod's priority. \\\"system-node-critical\\\" and \\\"system-cluster-critical\\\" are two special keywords which indicate the highest priorities with the former being the highest priority. Any other name must be defined by creating a PriorityClass object with that name. If not specified, the pod priority will be default or zero if there is no default.\",\n         \"type\": \"string\"\n        },\n        \"tolerations\": {\n         \"description\": \"If specified, the pod's tolerations.\\n\\nSee https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#toleration-v1-core\",\n         \"items\": {\n          \"description\": \"The pod this Toleration is attached to tolerates any taint that matches the triple \\u003ckey,value,effect\\u003e using the matching operator \\u003coperator\\u003e.\",\n          \"properties\": {\n           \"effect\": {\n            \"description\": \"Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute.\",\n            \"type\": \"string\"\n           },\n           \"key\": {\n            \"description\": \"Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys.\",\n            \"type\": \"string\"\n           },\n           \"operator\": {\n            \"description\": \"Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category.\",\n            \"type\": \"string\"\n           },\n           \"tolerationSeconds\": {\n            \"description\": \"TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system.\",\n            \"format\": \"int64\",\n            \"type\": \"integer\"\n           },\n           \"value\": {\n            \"description\": \"Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string.\",\n            \"type\": \"string\"\n           }\n          },\n          \"type\": \"object\"\n         },\n         \"type\": \"array\"\n        },\n        \"topologySpreadConstraints\": {\n         \"description\": \"TopologySpreadConstraints describes how a group of pods ought to spread across topology domains. Scheduler will schedule pods in a way which abides by the constraints. All topologySpreadConstraints are ANDed.\\n\\nSee https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#topologyspreadconstraint-v1-core\",\n         \"items\": {\n          \"description\": \"TopologySpreadConstraint specifies how to spread matching pods among the given topology.\",\n          \"properties\": {\n           \"labelSelector\": {\n            \"description\": \"A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects.\",\n            \"properties\": {\n             \"matchExpressions\": {\n              \"description\": \"matchExpressions is a list of label selector requirements. The requirements are ANDed.\",\n              \"items\": {\n               \"description\": \"A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.\",\n               \"properties\": {\n                \"key\": {\n                 \"description\": \"key is the label key that the selector applies to.\",\n                 \"type\": \"string\"\n                },\n                \"operator\": {\n                 \"description\": \"operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.\",\n                 \"type\": \"string\"\n                },\n                \"values\": {\n                 \"description\": \"values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.\",\n                 \"items\": {\n                  \"type\": \"string\"\n                 },\n                 \"type\": \"array\"\n                }\n               },\n               \"required\": [\n                \"key\",\n                \"operator\"\n               ],\n               \"type\": \"object\"\n              },\n              \"type\": \"array\"\n             },\n             \"matchLabels\": {\n              \"additionalProperties\": {\n               \"type\": \"string\"\n              },\n              \"description\": \"matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \\\"key\\\", the operator is \\\"In\\\", and the values array contains only \\\"value\\\". The requirements are ANDed.\",\n              \"type\": \"object\"\n             }\n            },\n            \"type\": \"object\"\n           },\n           \"matchLabelKeys\": {\n            \"description\": \"MatchLabelKeys is a set of pod label keys to select the pods over which spreading will be calculated. The keys are used to lookup values from the incoming pod labels, those key-value labels are ANDed with labelSelector to select the group of existing pods over which spreading will be calculated for the incoming pod. The same key is forbidden to exist in both MatchLabelKeys and LabelSelector. MatchLabelKeys cannot be set when LabelSelector isn't set. Keys that don't exist in the incoming pod labels will be ignored. A null or empty list means only match against labelSelector.\\n\\nThis is a beta field and requires the MatchLabelKeysInPodTopologySpread feature gate to be enabled (enabled by default).\",\n            \"items\": {\n             \"type\": \"string\"\n            },\n            \"type\": \"array\"\n           },\n           \"maxSkew\": {\n            \"description\": \"MaxSkew describes the degree to which pods may be unevenly distributed. When `whenUnsatisfiable=DoNotSchedule`, it is the maximum permitted difference between the number of matching pods in the target topology and the global minimum. The global minimum is the minimum number of matching pods in an eligible domain or zero if the number of eligible domains is less than MinDomains. For example, in a 3-zone cluster, MaxSkew is set to 1, and pods with the same labelSelector spread as 2/2/1: In this case, the global minimum is 1. | zone1 | zone2 | zone3 | |  P P  |  P P  |   P   | - if MaxSkew is 1, incoming pod can only be scheduled to zone3 to become 2/2/2; scheduling it onto zone1(zone2) would make the ActualSkew(3-1) on zone1(zone2) violate MaxSkew(1). - if MaxSkew is 2, incoming pod can be scheduled onto any zone. When `whenUnsatisfiable=ScheduleAnyway`, it is used to give higher precedence to topologies that satisfy it. It's a required field. Default value is 1 and 0 is not allowed.\",\n            \"format\": \"int32\",\n            \"type\": \"integer\"\n           },\n           \"minDomains\": {\n            \"description\": \"MinDomains indicates a minimum number of eligible domains. When the number of eligible domains with matching topology keys is less than minDomains, Pod Topology Spread treats \\\"global minimum\\\" as 0, and then the calculation of Skew is performed. And when the number of eligible domains with matching topology keys equals or greater than minDomains, this value has no effect on scheduling. As a result, when the number of eligible domains is less than minDomains, scheduler won't schedule more than maxSkew Pods to those domains. If value is nil, the constraint behaves as if MinDomains is equal to 1. Valid values are integers greater than 0. When value is not nil, WhenUnsatisfiable must be DoNotSchedule.\\n\\nFor example, in a 3-zone cluster, MaxSkew is set to 2, MinDomains is set to 5 and pods with the same labelSelector spread as 2/2/2: | zone1 | zone2 | zone3 | |  P P  |  P P  |  P P  | The number of domains is less than 5(MinDomains), so \\\"global minimum\\\" is treated as 0. In this situation, new pod with the same labelSelector cannot be scheduled, because computed skew will be 3(3 - 0) if new Pod is scheduled to any of the three zones, it will violate MaxSkew.\\n\\nThis is a beta field and requires the MinDomainsInPodTopologySpread feature gate to be enabled (enabled by default).\",\n            \"format\": \"int32\",\n            \"type\": \"integer\"\n           },\n           \"nodeAffinityPolicy\": {\n            \"description\": \"NodeAffinityPolicy indicates how we will treat Pod's nodeAffinity/nodeSelector when calculating pod topology spread skew. Options are: - Honor: only nodes matching nodeAffinity/nodeSelector are included in the calculations. - Ignore: nodeAffinity/nodeSelector are ignored. All nodes are included in the calculations.\\n\\nIf this value is nil, the behavior is equivalent to the Honor policy. This is a beta-level feature default enabled by the NodeInclusionPolicyInPodTopologySpread feature flag.\",\n            \"type\": \"string\"\n           },\n           \"nodeTaintsPolicy\": {\n            \"description\": \"NodeTaintsPolicy indicates how we will treat node taints when calculating pod topology spread skew. Options are: - Honor: nodes without taints, along with tainted nodes for which the incoming pod has a toleration, are included. - Ignore: node taints are ignored. All nodes are included.\\n\\nIf this value is nil, the behavior is equivalent to the Ignore policy. This is a beta-level feature default enabled by the NodeInclusionPolicyInPodTopologySpread feature flag.\",\n            \"type\": \"string\"\n           },\n           \"topologyKey\": {\n            \"description\": \"TopologyKey is the key of node labels. Nodes that have a label with this key and identical values are considered to be in the same topology. We consider each \\u003ckey, value\\u003e as a \\\"bucket\\\", and try to put balanced number of pods into each bucket. We define a domain as a particular instance of a topology. Also, we define an eligible domain as a domain whose nodes meet the requirements of nodeAffinityPolicy and nodeTaintsPolicy. e.g. If TopologyKey is \\\"kubernetes.io/hostname\\\", each Node is a domain of that topology. And, if TopologyKey is \\\"topology.kubernetes.io/zone\\\", each zone is a domain of that topology. It's a required field.\",\n            \"type\": \"string\"\n           },\n           \"whenUnsatisfiable\": {\n            \"description\": \"WhenUnsatisfiable indicates how to deal with a pod if it doesn't satisfy the spread constraint. - DoNotSchedule (default) tells the scheduler not to schedule it. - ScheduleAnyway tells the scheduler to schedule the pod in any location,\\n  but giving higher precedence to topologies that would help reduce the\\n  skew.\\nA constraint is considered \\\"Unsatisfiable\\\" for an incoming pod if and only if every possible node assignment for that pod would violate \\\"MaxSkew\\\" on some topology. For example, in a 3-zone cluster, MaxSkew is set to 1, and pods with the same labelSelector spread as 3/1/1: | zone1 | zone2 | zone3 | | P P P |   P   |   P   | If WhenUnsatisfiable is set to DoNotSchedule, incoming pod can only be scheduled to zone2(zone3) to become 3/2/1(3/1/2) as ActualSkew(2-1) on zone2(zone3) satisfies MaxSkew(1). In other words, the cluster can still be imbalanced, but scheduler won't make it *more* imbalanced. It's a required field.\",\n            \"type\": \"string\"\n           }\n          },\n          \"required\": [\n           \"maxSkew\",\n           \"topologyKey\",\n           \"whenUnsatisfiable\"\n          ],\n          \"type\": \"object\"\n         },\n         \"type\": \"array\"\n        }\n       },\n       \"type\": \"object\"\n      }\n     },\n     \"required\": [\n      \"persistentVolume\"\n     ],\n     \"type\": \"object\"\n    },\n    \"source\": {\n     \"description\": \"The data source of the stream to which change data capture will be applied. \\n\",\n     \"properties\": {\n      \"postgres\": {\n       \"description\": \"The configuration of the data source required when type is `Postgres`.\\n\",\n       \"properties\": {\n        \"database\": {\n         \"description\": \"The target database name to which the CDC process will connect to.\\n\\nIf not specified the default postgres database will be targeted.\\n\",\n         \"type\": \"string\"\n        },\n        \"debeziumProperties\": {\n         \"description\": \"Specific property of the debezium Postgres connector.\\n\\nSee https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-connector-properties\\n\\nEach property is converted from myPropertyName to my.property.name\\n\",\n         \"properties\": {\n          \"binaryHandlingMode\": {\n           \"description\": \"Default `bytes`. Specifies how binary (bytea) columns should be represented in change events:\\n\\n* `bytes` represents binary data as byte array.\\n* `base64` represents binary data as base64-encoded strings.\\n* `base64-url-safe` represents binary data as base64-url-safe-encoded strings.\\n* `hex` represents binary data as hex-encoded (base16) strings.\\n\",\n           \"type\": \"string\"\n          },\n          \"columnMaskHash\": {\n           \"additionalProperties\": {\n            \"additionalProperties\": {\n             \"additionalProperties\": {\n              \"description\": \"The list of regular expressions that match the fully-qualified names of character-based columns (e.g. inventory.orders.customerName)\",\n              \"items\": {\n               \"type\": \"string\"\n              },\n              \"type\": \"array\"\n             },\n             \"description\": \"The salt (e.g. CzQMA0cB5K) value and configuration.\",\n             \"type\": \"object\"\n            },\n            \"description\": \"The hash algorithm (e.g. SHA-256) type and configuration.\",\n            \"type\": \"object\"\n           },\n           \"description\": \"An optional section, that allow to specify, for an hash algorithm and a salt, a list of regular expressions that match the fully-qualified names of character-based columns. Fully-qualified names for columns are of the form \\u003cschemaName\\u003e.\\u003ctableName\\u003e.\\u003ccolumnName\\u003e.\\n To match the name of a column Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; the expression does not match substrings that might be present in a column name. In the resulting change event record, the values for the specified columns are replaced with pseudonyms.\\n A pseudonym consists of the hashed value that results from applying the specified hashAlgorithm and salt. Based on the hash function that is used, referential integrity is maintained, while column values are replaced with pseudonyms. Supported hash functions are described in the [MessageDigest section](https://docs.oracle.com/javase/7/docs/technotes/guides/security/StandardNames.html#MessageDigest) of the Java Cryptography Architecture Standard Algorithm Name Documentation.\\n In the following example, CzQMA0cB5K is a randomly selected salt.\\n columnMaskHash.SHA-256.CzQMA0cB5K=[inventory.orders.customerName,inventory.shipment.customerName]\\n If necessary, the pseudonym is automatically shortened to the length of the column. The connector configuration can include multiple properties that specify different hash algorithms and salts.\\n Depending on the hash algorithm used, the salt selected, and the actual data set, the resulting data set might not be completely masked.\\n\",\n           \"type\": \"object\"\n          },\n          \"columnMaskHashV2\": {\n           \"additionalProperties\": {\n            \"additionalProperties\": {\n             \"additionalProperties\": {\n              \"description\": \"The list of regular expressions that match the fully-qualified names of character-based columns (e.g. inventory.orders.customerName)\",\n              \"items\": {\n               \"type\": \"string\"\n              },\n              \"type\": \"array\"\n             },\n             \"description\": \"The salt (e.g. CzQMA0cB5K) value and configuration.\",\n             \"type\": \"object\"\n            },\n            \"description\": \"The hash algorithm (e.g. SHA-256) type and configuration.\",\n            \"type\": \"object\"\n           },\n           \"description\": \"Similar to also columnMaskHash but using hashing strategy version 2.\\n Hashing strategy version 2 should be used to ensure fidelity if the value is being hashed in different places or systems.\\n\",\n           \"type\": \"object\"\n          },\n          \"columnMaskWithLengthChars\": {\n           \"description\": \"An optional, list of regular expressions that match the fully-qualified names of character-based columns. Set this property if you want the connector to mask the values for a set of columns, for example, if they contain sensitive data. Set length to a positive integer to replace data in the specified columns with the number of asterisk (*) characters specified by the length in the property name. Set length to 0 (zero) to replace data in the specified columns with an empty string.\\n The fully-qualified name of a column observes the following format: schemaName.tableName.columnName.\\n To match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; the expression does not match substrings that might be present in a column name.\\n You can specify multiple properties with different lengths in a single configuration.\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"columnPropagateSourceType\": {\n           \"description\": \"Default `[.*]`. An optional, list of regular expressions that match the fully-qualified names of columns for which you want the connector to emit extra parameters that represent column metadata. When this property is set, the connector adds the following fields to the schema of event records:\\n\\n* `__debezium.source.column.type`\\n* `__debezium.source.column.length`\\n* `__debezium.source.column.scale`\\n\\nThese parameters propagate a columnâ€™s original type name and length (for variable-width types), respectively.\\n Enabling the connector to emit this extra data can assist in properly sizing specific numeric or character-based columns in sink databases.\\n The fully-qualified name of a column observes one of the following formats: databaseName.tableName.columnName, or databaseName.schemaName.tableName.columnName.\\n To match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; the expression does not match substrings that might be present in a column name.\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"columnTruncateToLengthChars\": {\n           \"description\": \"An optional, list of regular expressions that match the fully-qualified names of character-based columns. Set this property if you want to truncate the data in a set of columns when it exceeds the number of characters specified by the length in the property name. Set length to a positive integer value, for example, column.truncate.to.20.chars.\\n The fully-qualified name of a column observes the following format: \\u003cschemaName\\u003e.\\u003ctableName\\u003e.\\u003ccolumnName\\u003e.\\n To match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; the expression does not match substrings that might be present in a column name.\\n You can specify multiple properties with different lengths in a single configuration.\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"converters\": {\n           \"additionalProperties\": {\n            \"additionalProperties\": {\n             \"type\": \"string\"\n            },\n            \"type\": \"object\"\n           },\n           \"description\": \"Enumerates a comma-separated list of the symbolic names of the [custom converter](https://debezium.io/documentation/reference/stable/development/converters.html#custom-converters) instances that the connector can use. For example,\\n\\n```\\nisbn:\\n  type: io.debezium.test.IsbnConverter\\n  schemaName: io.debezium.postgresql.type.Isbn\\n```\\n\\nYou must set the converters property to enable the connector to use a custom converter.\\n For each converter that you configure for a connector, you must also add a .type property, which specifies the fully-qualified name of the class that implements the converter interface.\\nIf you want to further control the behavior of a configured converter, you can add one or more configuration parameters to pass values to the converter. To associate any additional configuration parameter with a converter, prefix the parameter names with the symbolic name of the converter.\\n Each property is converted from myPropertyName to my.property.name\\n\",\n           \"type\": \"object\"\n          },\n          \"customMetricTags\": {\n           \"additionalProperties\": {\n            \"type\": \"string\"\n           },\n           \"description\": \"The custom metric tags will accept key-value pairs to customize the MBean object name which should be appended the end of regular name, each key would represent a tag for the MBean object name, and the corresponding value would be the value of that tag the key is. For example:\\n\\n```\\ncustomMetricTags:\\n  k1: v1\\n  k2: v2\\n```\\n\",\n           \"type\": \"object\"\n          },\n          \"databaseInitialStatements\": {\n           \"description\": \"A list of SQL statements that the connector executes when it establishes a JDBC connection to the database.\\n The connector may establish JDBC connections at its own discretion. Consequently, this property is useful for configuration of session parameters only, and not for executing DML statements.\\n The connector does not execute these statements when it creates a connection for reading the transaction log.\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"datatypePropagateSourceType\": {\n           \"description\": \"Default `[.*]`. An optional, list of regular expressions that specify the fully-qualified names of data types that are defined for columns in a database. When this property is set, for columns with matching data types, the connector emits event records that include the following extra fields in their schema:\\n\\n* `__debezium.source.column.type`\\n* `__debezium.source.column.length`\\n* `__debezium.source.column.scale`\\n\\nThese parameters propagate a columnâ€™s original type name and length (for variable-width types), respectively.\\n Enabling the connector to emit this extra data can assist in properly sizing specific numeric or character-based columns in sink databases.\\n The fully-qualified name of a column observes one of the following formats: databaseName.tableName.typeName, or databaseName.schemaName.tableName.typeName.\\n To match the name of a data type, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the data type; the expression does not match substrings that might be present in a type name.\\n For the list of PostgreSQL-specific data type names, see the [PostgreSQL data type mappings](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-data-types).\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"decimalHandlingMode\": {\n           \"description\": \"Default `precise`. Specifies how the connector should handle values for DECIMAL and NUMERIC columns:\\n\\n* `precise`: represents values by using java.math.BigDecimal to represent values in binary form in change events.\\n* `double`: represents values by using double values, which might result in a loss of precision but which is easier to use.\\n* `string`: encodes values as formatted strings, which are easy to consume but semantic information about the real type is lost. For more information, see [Decimal types](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-decimal-types).\\n\",\n           \"type\": \"string\"\n          },\n          \"errorsMaxRetries\": {\n           \"description\": \"Default `-1`. Specifies how the connector responds after an operation that results in a retriable error, such as a connection error.\\n\\nSet one of the following options:\\n\\n* `-1`: No limit. The connector always restarts automatically, and retries the operation, regardless of the number of previous failures.\\n* `0`: Disabled. The connector fails immediately, and never retries the operation. User intervention is required to restart the connector.\\n* `\\u003e 0`: The connector restarts automatically until it reaches the specified maximum number of retries. After the next failure, the connector stops, and user intervention is required to restart it.\\n\",\n           \"type\": \"integer\"\n          },\n          \"eventProcessingFailureHandlingMode\": {\n           \"description\": \"Default `fail`. Specifies how the connector should react to exceptions during processing of events:\\n\\n* `fail`: propagates the exception, indicates the offset of the problematic event, and causes the connector to stop.\\n* `warn`: logs the offset of the problematic event, skips that event, and continues processing.\\n* `skip`: skips the problematic event and continues processing.\\n\",\n           \"type\": \"string\"\n          },\n          \"fieldNameAdjustmentMode\": {\n           \"description\": \"Default `none`. Specifies how field names should be adjusted for compatibility with the message converter used by the connector. Possible settings:\\n\\n* `none` does not apply any adjustment.\\n* `avro` replaces the characters that cannot be used in the Avro type name with underscore.\\n* `avro_unicode` replaces the underscore or characters that cannot be used in the Avro type name with corresponding unicode like _uxxxx. Note: _ is an escape sequence like backslash in Java\\n\\nFor more information, see [Avro naming](https://debezium.io/documentation/reference/stable/configuration/avro.html#avro-naming).\\n\",\n           \"type\": \"string\"\n          },\n          \"flushLsnSource\": {\n           \"description\": \"Default `true`. Determines whether the connector should commit the LSN of the processed records in the source postgres database so that the WAL logs can be deleted. Specify false if you donâ€™t want the connector to do this. Please note that if set to false LSN will not be acknowledged by Debezium and as a result WAL logs will not be cleared which might result in disk space issues. User is expected to handle the acknowledgement of LSN outside Debezium.\\n\",\n           \"type\": \"boolean\"\n          },\n          \"heartbeatActionQuery\": {\n           \"description\": \"Specifies a query that the connector executes on the source database when the connector sends a heartbeat message.\\n This is useful for resolving the situation described in [WAL disk space consumption](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-wal-disk-space), where capturing changes from a low-traffic database on the same host as a high-traffic database prevents Debezium from processing WAL records and thus acknowledging WAL positions with the database. To address this situation, create a heartbeat table in the low-traffic database, and set this property to a statement that inserts records into that table, for example:\\n\\n ```\\n INSERT INTO test_heartbeat_table (text) VALUES ('test_heartbeat')\\n ```\\n \\n This allows the connector to receive changes from the low-traffic database and acknowledge their LSNs, which prevents unbounded WAL growth on the database host.\\n\",\n           \"type\": \"string\"\n          },\n          \"heartbeatIntervalMs\": {\n           \"description\": \"Default `0`. Controls how frequently the connector sends heartbeat messages to a Kafka topic. The default behavior is that the connector does not send heartbeat messages.\\n Heartbeat messages are useful for monitoring whether the connector is receiving change events from the database. Heartbeat messages might help decrease the number of change events that need to be re-sent when a connector restarts. To send heartbeat messages, set this property to a positive integer, which indicates the number of milliseconds between heartbeat messages.\\n Heartbeat messages are needed when there are many updates in a database that is being tracked but only a tiny number of updates are related to the table(s) and schema(s) for which the connector is capturing changes. In this situation, the connector reads from the database transaction log as usual but rarely emits change records to Kafka. This means that no offset updates are committed to Kafka and the connector does not have an opportunity to send the latest retrieved LSN to the database. The database retains WAL files that contain events that have already been processed by the connector. Sending heartbeat messages enables the connector to send the latest retrieved LSN to the database, which allows the database to reclaim disk space being used by no longer needed WAL files.\\n\",\n           \"type\": \"integer\"\n          },\n          \"hstoreHandlingMode\": {\n           \"description\": \"Default `json`. Specifies how the connector should handle values for hstore columns:\\n\\n* `map`: represents values by using MAP.\\n* `json`: represents values by using json string. This setting encodes values as formatted strings such as {\\\"key\\\" : \\\"val\\\"}. For more information, see [PostgreSQL HSTORE type](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-hstore-type).\\n\",\n           \"type\": \"string\"\n          },\n          \"includeUnknownDatatypes\": {\n           \"description\": \"Default `true`. Specifies connector behavior when the connector encounters a field whose data type is unknown. The default behavior is that the connector omits the field from the change event and logs a warning.\\n Set this property to true if you want the change event to contain an opaque binary representation of the field. This lets consumers decode the field. You can control the exact representation by setting the binaryHandlingMode property.\\n\\u003e *NOTE*: Consumers risk backward compatibility issues when `includeUnknownDatatypes` is set to `true`. Not only may the database-specific binary representation change between releases, but if the data type is eventually supported by Debezium, the data type will be sent downstream in a logical type, which would require adjustments by consumers. In general, when encountering unsupported data types, create a feature request so that support can be added.\\n\",\n           \"type\": \"boolean\"\n          },\n          \"incrementalSnapshotChunkSize\": {\n           \"description\": \"Default `1024`. The maximum number of rows that the connector fetches and reads into memory during an incremental snapshot chunk. Increasing the chunk size provides greater efficiency, because the snapshot runs fewer snapshot queries of a greater size. However, larger chunk sizes also require more memory to buffer the snapshot data. Adjust the chunk size to a value that provides the best performance in your environment.\\n\",\n           \"type\": \"integer\"\n          },\n          \"incrementalSnapshotWatermarkingStrategy\": {\n           \"description\": \"Default `insert_insert`. Specifies the watermarking mechanism that the connector uses during an incremental snapshot to deduplicate events that might be captured by an incremental snapshot and then recaptured after streaming resumes.\\n\\nYou can specify one of the following options:\\n\\n* `insert_insert`: When you send a signal to initiate an incremental snapshot, for every chunk that Debezium reads during the snapshot, it writes an entry to the signaling data collection to record the signal to open the snapshot window. After the snapshot completes, Debezium inserts a second entry to record the closing of the window.\\n* `insert_delete`: When you send a signal to initiate an incremental snapshot, for every chunk that Debezium reads, it writes a single entry to the signaling data collection to record the signal to open the snapshot window. After the snapshot completes, this entry is removed. No entry is created for the signal to close the snapshot window. Set this option to prevent rapid growth of the signaling data collection.\\n\",\n           \"type\": \"string\"\n          },\n          \"intervalHandlingMode\": {\n           \"description\": \"Default `numeric`. Specifies how the connector should handle values for interval columns:\\n\\n * `numeric`: represents intervals using approximate number of microseconds.\\n * `string`: represents intervals exactly by using the string pattern representation P\\u003cyears\\u003eY\\u003cmonths\\u003eM\\u003cdays\\u003eDT\\u003chours\\u003eH\\u003cminutes\\u003eM\\u003cseconds\\u003eS. For example: P1Y2M3DT4H5M6.78S. For more information, see [PostgreSQL basic types](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-basic-types).\\n\",\n           \"type\": \"string\"\n          },\n          \"maxBatchSize\": {\n           \"description\": \"Default `2048`. Positive integer value that specifies the maximum size of each batch of events that the connector processes.\\n\",\n           \"type\": \"integer\"\n          },\n          \"maxQueueSize\": {\n           \"description\": \"Default `8192`. Positive integer value that specifies the maximum number of records that the blocking queue can hold. When Debezium reads events streamed from the database, it places the events in the blocking queue before it writes them to Kafka. The blocking queue can provide backpressure for reading change events from the database in cases where the connector ingests messages faster than it can write them to Kafka, or when Kafka becomes unavailable. Events that are held in the queue are disregarded when the connector periodically records offsets. Always set the value of maxQueueSize to be larger than the value of maxBatchSize.\\n\",\n           \"type\": \"integer\"\n          },\n          \"maxQueueSizeInBytes\": {\n           \"description\": \"Default `0`. A long integer value that specifies the maximum volume of the blocking queue in bytes. By default, volume limits are not specified for the blocking queue. To specify the number of bytes that the queue can consume, set this property to a positive long value.\\n If maxQueueSize is also set, writing to the queue is blocked when the size of the queue reaches the limit specified by either property. For example, if you set maxQueueSize=1000, and maxQueueSizeInBytes=5000, writing to the queue is blocked after the queue contains 1000 records, or after the volume of the records in the queue reaches 5000 bytes.\\n\",\n           \"type\": \"integer\"\n          },\n          \"messageKeyColumns\": {\n           \"description\": \"A list of expressions that specify the columns that the connector uses to form custom message keys for change event records that it publishes to the Kafka topics for specified tables.\\n By default, Debezium uses the primary key column of a table as the message key for records that it emits. In place of the default, or to specify a key for tables that lack a primary key, you can configure custom message keys based on one or more columns.\\n To establish a custom message key for a table, list the table, followed by the columns to use as the message key. Each list entry takes the following format:\\n \\u003cfully-qualified_tableName\\u003e:\\u003ckeyColumn\\u003e,\\u003ckeyColumn\\u003e\\n To base a table key on multiple column names, insert commas between the column names.\\n Each fully-qualified table name is a regular expression in the following format:\\n \\u003cschemaName\\u003e.\\u003ctableName\\u003e\\n The property can include entries for multiple tables. Use a semicolon to separate table entries in the list.\\n The following example sets the message key for the tables inventory.customers and purchase.orders:\\n inventory.customers:pk1,pk2;(.*).purchaseorders:pk3,pk4\\n For the table inventory.customer, the columns pk1 and pk2 are specified as the message key. For the purchaseorders tables in any schema, the columns pk3 and pk4 server as the message key.\\n There is no limit to the number of columns that you use to create custom message keys. However, itâ€™s best to use the minimum number that are required to specify a unique key.\\n Note that having this property set and REPLICA IDENTITY set to DEFAULT on the tables, will cause the tombstone events to not be created properly if the key columns are not part of the primary key of the table. Setting REPLICA IDENTITY to FULL is the only solution.\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"moneyFractionDigits\": {\n           \"description\": \"Default `2`. Specifies how many decimal digits should be used when converting Postgres money type to java.math.BigDecimal, which represents the values in change events. Applicable only when decimalHandlingMode is set to precise.\\n\",\n           \"type\": \"integer\"\n          },\n          \"notificationEnabledChannels\": {\n           \"description\": \"List of notification channel names that are enabled for the connector. By default, the following channels are available: sink, log and jmx. Optionally, you can also implement a [custom notification channel](https://debezium.io/documentation/reference/stable/configuration/signalling.html#debezium-signaling-enabling-custom-signaling-channel).\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"pluginName\": {\n           \"description\": \"Default `pgoutput`. The name of the [PostgreSQL logical decoding plug-in](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-output-plugin) installed on the PostgreSQL server. Supported values are decoderbufs, and pgoutput.\\n\",\n           \"type\": \"string\"\n          },\n          \"pollIntervalMs\": {\n           \"description\": \"Default `500`. Positive integer value that specifies the number of milliseconds the connector should wait for new change events to appear before it starts processing a batch of events. Defaults to 500 milliseconds.\\n\",\n           \"type\": \"integer\"\n          },\n          \"provideTransactionMetadata\": {\n           \"description\": \"Default `false`. Determines whether the connector generates events with transaction boundaries and enriches change event envelopes with transaction metadata. Specify true if you want the connector to do this. For more information, see [Transaction metadata](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-transaction-metadata).\\n\",\n           \"type\": \"boolean\"\n          },\n          \"publicationAutocreateMode\": {\n           \"description\": \"Default `all_tables`. Applies only when streaming changes by using [the pgoutput plug-in](https://www.postgresql.org/docs/current/sql-createpublication.html). The setting determines how creation of a [publication](https://www.postgresql.org/docs/current/logical-replication-publication.html) should work. Specify one of the following values:\\n\\n* `all_tables` - If a publication exists, the connector uses it. If a publication does not exist, the connector creates a publication for all tables in the database for which the connector is capturing changes. For the connector to create a publication it must access the database through a database user account that has permission to create publications and perform replications. You grant the required permission by using the following SQL command CREATE PUBLICATION \\u003cpublication_name\\u003e FOR ALL TABLES;.\\n* `disabled` - The connector does not attempt to create a publication. A database administrator or the user configured to perform replications must have created the publication before running the connector. If the connector cannot find the publication, the connector throws an exception and stops.\\n* `filtered` - If a publication exists, the connector uses it. If no publication exists, the connector creates a new publication for tables that match the current filter configuration as specified by the schema.include.list, schema.exclude.list, and table.include.list, and table.exclude.list connector configuration properties. For example: CREATE PUBLICATION \\u003cpublication_name\\u003e FOR TABLE \\u003ctbl1, tbl2, tbl3\\u003e. If the publication exists, the connector updates the publication for tables that match the current filter configuration. For example: ALTER PUBLICATION \\u003cpublication_name\\u003e SET TABLE \\u003ctbl1, tbl2, tbl3\\u003e.\\n\",\n           \"type\": \"string\"\n          },\n          \"publicationName\": {\n           \"description\": \"Default \\u003cSGStream name\\u003e.\\u003cSGStream namespace\\u003e (with all characters that are not `[a-zA-Z0-9]` changed to `_` character). The name of the PostgreSQL publication created for streaming changes when using pgoutput. This publication is created at start-up if it does not already exist and it includes all tables. Debezium then applies its own include/exclude list filtering, if configured, to limit the publication to change events for the specific tables of interest. The connector user must have superuser permissions to create this publication, so it is usually preferable to create the publication before starting the connector for the first time. If the publication already exists, either for all tables or configured with a subset of tables, Debezium uses the publication as it is defined.\\n\",\n           \"type\": \"string\"\n          },\n          \"replicaIdentityAutosetValues\": {\n           \"description\": \"The setting determines the value for [replica identity](https://www.postgresql.org/docs/current/sql-altertable.html#SQL-ALTERTABLE-REPLICA-IDENTITY) at table level.\\n  This option will overwrite the existing value in database. A comma-separated list of regular expressions that match fully-qualified tables and replica identity value to be used in the table.\\n  Each expression must match the pattern '\\u003cfully-qualified table name\\u003e:\\u003creplica identity\\u003e', where the table name could be defined as (SCHEMA_NAME.TABLE_NAME), and the replica identity values are:\\n  DEFAULT - Records the old values of the columns of the primary key, if any. This is the default for non-system tables.\\n  INDEX index_name - Records the old values of the columns covered by the named index, that must be unique, not partial, not deferrable, and include only columns marked NOT NULL. If this index is dropped, the behavior is the same as NOTHING.\\n  FULL - Records the old values of all columns in the row.\\n  NOTHING - Records no information about the old row. This is the default for system tables.\\n  For example,\\n  schema1.*:FULL,schema2.table2:NOTHING,schema2.table3:INDEX idx_name\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"retriableRestartConnectorWaitMs\": {\n           \"description\": \"Default `10000` (10 seconds). The number of milliseconds to wait before restarting a connector after a retriable error occurs.\\n\",\n           \"type\": \"integer\"\n          },\n          \"schemaNameAdjustmentMode\": {\n           \"description\": \"Default `none`. Specifies how schema names should be adjusted for compatibility with the message converter used by the connector. Possible settings:\\n\\n* `none` does not apply any adjustment.\\n* `avro` replaces the characters that cannot be used in the Avro type name with underscore.\\n* `avro_unicode` replaces the underscore or characters that cannot be used in the Avro type name with corresponding unicode like _uxxxx. Note: _ is an escape sequence like backslash in Java\\n\",\n           \"type\": \"string\"\n          },\n          \"schemaRefreshMode\": {\n           \"description\": \"Default `columns_diff`. Specify the conditions that trigger a refresh of the in-memory schema for a table.\\n\\n* `columns_diff`: is the safest mode. It ensures that the in-memory schema stays in sync with the database tableâ€™s schema at all times.\\n* `columns_diff_exclude_unchanged_toast`: instructs the connector to refresh the in-memory schema cache if there is a discrepancy with the schema derived from the incoming message, unless unchanged TOASTable data fully accounts for the discrepancy.\\n\\nThis setting can significantly improve connector performance if there are frequently-updated tables that have TOASTed data that are rarely part of updates. However, it is possible for the in-memory schema to become outdated if TOASTable columns are dropped from the table.\\n\",\n           \"type\": \"string\"\n          },\n          \"signalDataCollection\": {\n           \"description\": \"Fully-qualified name of the data collection that is used to send signals to the connector. Use the following format to specify the collection name: \\u003cschemaName\\u003e.\\u003ctableName\\u003e\\n\",\n           \"type\": \"string\"\n          },\n          \"signalEnabledChannels\": {\n           \"description\": \"Default `[sgstream-annotations]`. List of the signaling channel names that are enabled for the connector. By default, the following channels are available: sgstream-annotations, source, kafka, file and jmx. Optionally, you can also implement a [custom signaling channel](https://debezium.io/documentation/reference/stable/configuration/signalling.html#debezium-signaling-enabling-custom-signaling-channel).\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"skipMessagesWithoutChange\": {\n           \"description\": \"Default `false`. Specifies whether to skip publishing messages when there is no change in included columns. This would essentially filter messages if there is no change in columns included as per includes or excludes fields. Note: Only works when REPLICA IDENTITY of the table is set to FULL\\n\",\n           \"type\": \"boolean\"\n          },\n          \"skippedOperations\": {\n           \"description\": \"Default `none`. A list of operation types that will be skipped during streaming. The operations include: c for inserts/create, u for updates, d for deletes, t for truncates, and none to not skip any operations. By default, no operations are skipped.\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"slotDropOnStop\": {\n           \"description\": \"Default `true`. Whether or not to delete the logical replication slot when the connector stops in a graceful, expected way. The default behavior is that the replication slot remains configured for the connector when the connector stops. When the connector restarts, having the same replication slot enables the connector to start processing where it left off. Set to true in only testing or development environments. Dropping the slot allows the database to discard WAL segments. When the connector restarts it performs a new snapshot or it can continue from a persistent offset in the Kafka Connect offsets topic.\\n\",\n           \"type\": \"boolean\"\n          },\n          \"slotMaxRetries\": {\n           \"description\": \"Default `6`. If connecting to a replication slot fails, this is the maximum number of consecutive attempts to connect.\\n\",\n           \"type\": \"integer\"\n          },\n          \"slotName\": {\n           \"description\": \"Default \\u003cSGStream namespace\\u003e.\\u003cSGStream name\\u003e (with all characters that are not `[a-zA-Z0-9]` changed to `_` character). The name of the PostgreSQL logical decoding slot that was created for streaming changes from a particular plug-in for a particular database/schema. The server uses this slot to stream events to the Debezium connector that you are configuring.\\n\\nSlot names must conform to [PostgreSQL replication slot naming rules](https://www.postgresql.org/docs/current/static/warm-standby.html#STREAMING-REPLICATION-SLOTS-MANIPULATION), which state: \\\"Each replication slot has a name, which can contain lower-case letters, numbers, and the underscore character.\\\"\\n\",\n           \"type\": \"string\"\n          },\n          \"slotRetryDelayMs\": {\n           \"description\": \"Default `10000` (10 seconds). The number of milliseconds to wait between retry attempts when the connector fails to connect to a replication slot.\\n\",\n           \"type\": \"integer\"\n          },\n          \"slotStreamParams\": {\n           \"additionalProperties\": {\n            \"type\": \"string\"\n           },\n           \"description\": \"Parameters to pass to the configured logical decoding plug-in. For example:\\n\\n```\\nslotStreamParams:\\n  add-tables: \\\"public.table,public.table2\\\"\\n  include-lsn: \\\"true\\\"\\n```\\n\",\n           \"type\": \"object\"\n          },\n          \"snapshotDelayMs\": {\n           \"description\": \"An interval in milliseconds that the connector should wait before performing a snapshot when the connector starts. If you are starting multiple connectors in a cluster, this property is useful for avoiding snapshot interruptions, which might cause re-balancing of connectors.\\n\",\n           \"type\": \"integer\"\n          },\n          \"snapshotFetchSize\": {\n           \"description\": \"Default `10240`. During a snapshot, the connector reads table content in batches of rows. This property specifies the maximum number of rows in a batch.\\n\",\n           \"type\": \"integer\"\n          },\n          \"snapshotIncludeCollectionList\": {\n           \"description\": \"Default \\u003cAll tables / All tables filtered in `includes` field / All tables that are not filtered out in `excludes` field\\u003e. An optional, list of regular expressions that match the fully-qualified names (\\u003cschemaName\\u003e.\\u003ctableName\\u003e) of the tables to include in a snapshot. The specified items must be named in the connectorâ€™s table.include.list property. This property takes effect only if the connectorâ€™s snapshotMode property is set to a value other than `never`. This property does not affect the behavior of incremental snapshots.\\n   To match the name of a table, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the table; it does not match substrings that might be present in a table name.\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"snapshotLockTimeoutMs\": {\n           \"description\": \"Default `10000`. Positive integer value that specifies the maximum amount of time (in milliseconds) to wait to obtain table locks when performing a snapshot. If the connector cannot acquire table locks in this time interval, the snapshot fails. [How the connector performs snapshots](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-snapshots) provides details.\\n\",\n           \"type\": \"integer\"\n          },\n          \"snapshotLockingMode\": {\n           \"description\": \"Default `none`. Specifies how the connector holds locks on tables while performing a schema snapshot. Set one of the following options:\\n\\n* `shared`: The connector holds a table lock that prevents exclusive table access during the initial portion phase of the snapshot in which database schemas and other metadata are read. After the initial phase, the snapshot no longer requires table locks.\\n* `none`: The connector avoids locks entirely. Do not use this mode if schema changes might occur during the snapshot.\\n\\n\\u003e *WARNING*: Do not use this mode if schema changes might occur during the snapshot.\\n\\n* `custom`: The connector performs a snapshot according to the implementation specified by the snapshotLockingModeCustomName property, which is a custom implementation of the io.debezium.spi.snapshot.SnapshotLock interface.\\n\",\n           \"type\": \"string\"\n          },\n          \"snapshotLockingModeCustomName\": {\n           \"description\": \"When snapshotLockingMode is set to custom, use this setting to specify the name of the custom implementation provided in the name() method that is defined by the 'io.debezium.spi.snapshot.SnapshotLock' interface. For more information, see [custom snapshotter SPI](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#connector-custom-snapshot).\\n\",\n           \"type\": \"string\"\n          },\n          \"snapshotMaxThreads\": {\n           \"description\": \"Default `1`. Specifies the number of threads that the connector uses when performing an initial snapshot. To enable parallel initial snapshots, set the property to a value greater than 1. In a parallel initial snapshot, the connector processes multiple tables concurrently. This feature is incubating.\\n\",\n           \"type\": \"integer\"\n          },\n          \"snapshotMode\": {\n           \"description\": \"Default `initial`. Specifies the criteria for performing a snapshot when the connector starts:\\n\\n* `always` - The connector performs a snapshot every time that it starts. The snapshot includes the structure and data of the captured tables. Specify this value to populate topics with a complete representation of the data from the captured tables every time that the connector starts. After the snapshot completes, the connector begins to stream event records for subsequent database changes.\\n* `initial` - The connector performs a snapshot only when no offsets have been recorded for the logical server name.\\n* `initial_only` - The connector performs an initial snapshot and then stops, without processing any subsequent changes.\\n* `no_data` - The connector never performs snapshots. When a connector is configured this way, after it starts, it behaves as follows: If there is a previously stored LSN in the Kafka offsets topic, the connector continues streaming changes from that position. If no LSN is stored, the connector starts streaming changes from the point in time when the PostgreSQL logical replication slot was created on the server. Use this snapshot mode only when you know all data of interest is still reflected in the WAL.\\n* `never` - Deprecated see no_data.\\n* `when_needed` - After the connector starts, it performs a snapshot only if it detects one of the following circumstances: \\n  It cannot detect any topic offsets.\\n  A previously recorded offset specifies a log position that is not available on the server.\\n* `configuration_based` - With this option, you control snapshot behavior through a set of connector properties that have the prefix 'snapshotModeConfigurationBased'.\\n* `custom` - The connector performs a snapshot according to the implementation specified by the snapshotModeCustomName property, which defines a custom implementation of the io.debezium.spi.snapshot.Snapshotter interface.\\n\\nFor more information, see the [table of snapshot.mode options](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-connector-snapshot-mode-options).\\n\",\n           \"type\": \"string\"\n          },\n          \"snapshotModeConfigurationBasedSnapshotData\": {\n           \"description\": \"Default `false`. If the snapshotMode is set to configuration_based, set this property to specify whether the connector includes table data when it performs a snapshot.\\n\",\n           \"type\": \"boolean\"\n          },\n          \"snapshotModeConfigurationBasedSnapshotOnDataError\": {\n           \"description\": \"Default `false`. If the snapshotMode is set to configuration_based, this property specifies whether the connector attempts to snapshot table data if it does not find the last committed offset in the transaction log. Set the value to true to instruct the connector to perform a new snapshot.\\n\",\n           \"type\": \"boolean\"\n          },\n          \"snapshotModeConfigurationBasedSnapshotOnSchemaError\": {\n           \"description\": \"Default `false`. If the snapshotMode is set to configuration_based, set this property to specify whether the connector includes table schema in a snapshot if the schema history topic is not available.\\n\",\n           \"type\": \"boolean\"\n          },\n          \"snapshotModeConfigurationBasedSnapshotSchema\": {\n           \"description\": \"Default `false`. If the snapshotMode is set to configuration_based, set this property to specify whether the connector includes the table schema when it performs a snapshot.\\n\",\n           \"type\": \"boolean\"\n          },\n          \"snapshotModeConfigurationBasedStartStream\": {\n           \"description\": \"Default `false`. If the snapshotMode is set to configuration_based, set this property to specify whether the connector begins to stream change events after a snapshot completes.\\n\",\n           \"type\": \"boolean\"\n          },\n          \"snapshotModeCustomName\": {\n           \"description\": \"When snapshotMode is set as custom, use this setting to specify the name of the custom implementation provided in the name() method that is defined by the 'io.debezium.spi.snapshot.Snapshotter' interface. The provided implementation is called after a connector restart to determine whether to perform a snapshot. For more information, see [custom snapshotter SPI](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#connector-custom-snapshot).\\n\",\n           \"type\": \"string\"\n          },\n          \"snapshotQueryMode\": {\n           \"description\": \"Default `select_all`. Specifies how the connector queries data while performing a snapshot. Set one of the following options:\\n\\n* `select_all`: The connector performs a select all query by default, optionally adjusting the columns selected based on the column include and exclude list configurations.\\n* `custom`: The connector performs a snapshot query according to the implementation specified by the snapshotQueryModeCustomName property, which defines a custom implementation of the io.debezium.spi.snapshot.SnapshotQuery interface. This setting enables you to manage snapshot content in a more flexible manner compared to using the snapshotSelectStatementOverrides property.\\n\",\n           \"type\": \"string\"\n          },\n          \"snapshotQueryModeCustomName\": {\n           \"description\": \"When snapshotQueryMode is set as custom, use this setting to specify the name of the custom implementation provided in the name() method that is defined by the 'io.debezium.spi.snapshot.SnapshotQuery' interface. For more information, see [custom snapshotter SPI](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#connector-custom-snapshot).\\n\",\n           \"type\": \"string\"\n          },\n          \"snapshotSelectStatementOverrides\": {\n           \"additionalProperties\": {\n            \"type\": \"string\"\n           },\n           \"description\": \"Specifies the table rows to include in a snapshot. Use the property if you want a snapshot to include only a subset of the rows in a table. This property affects snapshots only. It does not apply to events that the connector reads from the log.\\n The property contains a hierarchy of fully-qualified table names in the form \\u003cschemaName\\u003e.\\u003ctableName\\u003e. For example,\\n\\n```\\nsnapshotSelectStatementOverrides: \\n  \\\"customers.orders\\\": \\\"SELECT * FROM [customers].[orders] WHERE delete_flag = 0 ORDER BY id DESC\\\"\\n```\\n\\nIn the resulting snapshot, the connector includes only the records for which delete_flag = 0.\\n\",\n           \"type\": \"object\"\n          },\n          \"statusUpdateIntervalMs\": {\n           \"description\": \"Default `10000`. Frequency for sending replication connection status updates to the server, given in milliseconds. The property also controls how frequently the database status is checked to detect a dead connection in case the database was shut down.\\n\",\n           \"type\": \"integer\"\n          },\n          \"timePrecisionMode\": {\n           \"description\": \"Default `adaptive`. Time, date, and timestamps can be represented with different kinds of precision:\\n\\n* `adaptive`: captures the time and timestamp values exactly as in the database using either millisecond, microsecond, or nanosecond precision values based on the database columnâ€™s type.\\n* `adaptive_time_microseconds`: captures the date, datetime and timestamp values exactly as in the database using either millisecond, microsecond, or nanosecond precision values based on the database columnâ€™s type. An exception is TIME type fields, which are always captured as microseconds.\\n* `connect`: always represents time and timestamp values by using Kafka Connectâ€™s built-in representations for Time, Date, and Timestamp, which use millisecond precision regardless of the database columns' precision. For more information, see [temporal values](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-temporal-types).\\n\",\n           \"type\": \"string\"\n          },\n          \"tombstonesOnDelete\": {\n           \"description\": \"Default `true`. Controls whether a delete event is followed by a tombstone event.\\n\\n* `true` - a delete operation is represented by a delete event and a subsequent tombstone event.\\n* `false` - only a delete event is emitted.\\n\\nAfter a source record is deleted, emitting a tombstone event (the default behavior) allows Kafka to completely delete all events that pertain to the key of the deleted row in case [log compaction](https://kafka.apache.org/documentation/#compaction) is enabled for the topic.\\n\",\n           \"type\": \"boolean\"\n          },\n          \"topicCacheSize\": {\n           \"description\": \"Default `10000`. The size used for holding the topic names in bounded concurrent hash map. This cache will help to determine the topic name corresponding to a given data collection.\\n\",\n           \"type\": \"integer\"\n          },\n          \"topicDelimiter\": {\n           \"description\": \"Default `.`. Specify the delimiter for topic name, defaults to \\\".\\\".\\n\",\n           \"type\": \"string\"\n          },\n          \"topicHeartbeatPrefix\": {\n           \"description\": \"Default `__debezium-heartbeat`. Controls the name of the topic to which the connector sends heartbeat messages. For example, if the topic prefix is fulfillment, the default topic name is __debezium-heartbeat.fulfillment.\\n\",\n           \"type\": \"string\"\n          },\n          \"topicNamingStrategy\": {\n           \"description\": \"Default `io.debezium.schema.SchemaTopicNamingStrategy`. The name of the TopicNamingStrategy class that should be used to determine the topic name for data change, schema change, transaction, heartbeat event etc., defaults to SchemaTopicNamingStrategy.\\n\",\n           \"type\": \"string\"\n          },\n          \"topicTransaction\": {\n           \"description\": \"Default `transaction`. Controls the name of the topic to which the connector sends transaction metadata messages. For example, if the topic prefix is fulfillment, the default topic name is fulfillment.transaction.\\n\",\n           \"type\": \"string\"\n          },\n          \"unavailableValuePlaceholder\": {\n           \"description\": \"Default `__debezium_unavailable_value`. Specifies the constant that the connector provides to indicate that the original value is a toasted value that is not provided by the database. If the setting of unavailable.value.placeholder starts with the hex: prefix it is expected that the rest of the string represents hexadecimally encoded octets. For more information, see [toasted values](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-toasted-values).\\n\",\n           \"type\": \"string\"\n          },\n          \"xminFetchIntervalMs\": {\n           \"description\": \"Default `0`. How often, in milliseconds, the XMIN will be read from the replication slot. The XMIN value provides the lower bounds of where a new replication slot could start from. The default value of 0 disables tracking XMIN tracking.\\n\",\n           \"type\": \"integer\"\n          }\n         },\n         \"type\": \"object\"\n        },\n        \"excludes\": {\n         \"description\": \"A list of regular expressions that allow to match one or more `\\u003cschema\\u003e.\\u003ctable\\u003e.\\u003ccolumn\\u003e` entries to be filtered out before sending to the target.\\n\\nThis property is mutually exclusive with `includes`.\\n\",\n         \"items\": {\n          \"description\": \"A regular expressions that allow to match one or more `\\u003cschema\\u003e.\\u003ctable\\u003e.\\u003ccolumn\\u003e` entries to be filtered out before sending to the target.\\n\",\n          \"type\": \"string\"\n         },\n         \"type\": \"array\"\n        },\n        \"host\": {\n         \"description\": \"The hostname of the Postgres instance.\\n\",\n         \"type\": \"string\"\n        },\n        \"includes\": {\n         \"description\": \"A list of regular expressions that allow to match one or more `\\u003cschema\\u003e.\\u003ctable\\u003e.\\u003ccolumn\\u003e` entries to be filtered before sending to the target.\\n\\nThis property is mutually exclusive with `excludes`.\\n\",\n         \"items\": {\n          \"description\": \"A regular expressions that allow to match one or more `\\u003cschema\\u003e.\\u003ctable\\u003e.\\u003ccolumn\\u003e` entries to be filtered before sending to the target.\\n\",\n          \"type\": \"string\"\n         },\n         \"type\": \"array\"\n        },\n        \"password\": {\n         \"description\": \"The password used by the CDC process to connect to the database.\\n\\nIf not specified the default superuser password will be used.\\n\",\n         \"properties\": {\n          \"key\": {\n           \"description\": \"The Secret key where the password is stored.\\n\",\n           \"type\": \"string\"\n          },\n          \"name\": {\n           \"description\": \"The Secret name where the password is stored.\\n\",\n           \"type\": \"string\"\n          }\n         },\n         \"required\": [\n          \"name\",\n          \"key\"\n         ],\n         \"type\": \"object\"\n        },\n        \"port\": {\n         \"description\": \"The port of the Postgres instance. When not specified port 5432 will be used.\\n\",\n         \"type\": \"integer\"\n        },\n        \"username\": {\n         \"description\": \"The username used by the CDC process to connect to the database.\\n\\nIf not specified the default superuser username (by default postgres) will be used.\\n\",\n         \"properties\": {\n          \"key\": {\n           \"description\": \"The Secret key where the username is stored.\\n\",\n           \"type\": \"string\"\n          },\n          \"name\": {\n           \"description\": \"The Secret name where the username is stored.\\n\",\n           \"type\": \"string\"\n          }\n         },\n         \"required\": [\n          \"name\",\n          \"key\"\n         ],\n         \"type\": \"object\"\n        }\n       },\n       \"required\": [\n        \"host\"\n       ],\n       \"type\": \"object\"\n      },\n      \"sgCluster\": {\n       \"description\": \"The configuration of the data source required when type is `SGCluster`.\\n\",\n       \"properties\": {\n        \"database\": {\n         \"description\": \"The target database name to which the CDC process will connect to.\\n\\nIf not specified the default postgres database will be targeted.\\n\",\n         \"type\": \"string\"\n        },\n        \"debeziumProperties\": {\n         \"description\": \"Specific property of the debezium Postgres connector.\\n\\nSee https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-connector-properties\\n\\nEach property is converted from myPropertyName to my.property.name\\n\",\n         \"properties\": {\n          \"binaryHandlingMode\": {\n           \"description\": \"Default `bytes`. Specifies how binary (bytea) columns should be represented in change events:\\n\\n* `bytes` represents binary data as byte array.\\n* `base64` represents binary data as base64-encoded strings.\\n* `base64-url-safe` represents binary data as base64-url-safe-encoded strings.\\n* `hex` represents binary data as hex-encoded (base16) strings.\\n\",\n           \"type\": \"string\"\n          },\n          \"columnMaskHash\": {\n           \"additionalProperties\": {\n            \"additionalProperties\": {\n             \"additionalProperties\": {\n              \"description\": \"The list of regular expressions that match the fully-qualified names of character-based columns (e.g. inventory.orders.customerName)\",\n              \"items\": {\n               \"type\": \"string\"\n              },\n              \"type\": \"array\"\n             },\n             \"description\": \"The salt (e.g. CzQMA0cB5K) value and configuration.\",\n             \"type\": \"object\"\n            },\n            \"description\": \"The hash algorithm (e.g. SHA-256) type and configuration.\",\n            \"type\": \"object\"\n           },\n           \"description\": \"An optional section, that allow to specify, for an hash algorithm and a salt, a list of regular expressions that match the fully-qualified names of character-based columns. Fully-qualified names for columns are of the form \\u003cschemaName\\u003e.\\u003ctableName\\u003e.\\u003ccolumnName\\u003e.\\n To match the name of a column Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; the expression does not match substrings that might be present in a column name. In the resulting change event record, the values for the specified columns are replaced with pseudonyms.\\n A pseudonym consists of the hashed value that results from applying the specified hashAlgorithm and salt. Based on the hash function that is used, referential integrity is maintained, while column values are replaced with pseudonyms. Supported hash functions are described in the [MessageDigest section](https://docs.oracle.com/javase/7/docs/technotes/guides/security/StandardNames.html#MessageDigest) of the Java Cryptography Architecture Standard Algorithm Name Documentation.\\n In the following example, CzQMA0cB5K is a randomly selected salt.\\n columnMaskHash.SHA-256.CzQMA0cB5K=[inventory.orders.customerName,inventory.shipment.customerName]\\n If necessary, the pseudonym is automatically shortened to the length of the column. The connector configuration can include multiple properties that specify different hash algorithms and salts.\\n Depending on the hash algorithm used, the salt selected, and the actual data set, the resulting data set might not be completely masked.\\n\",\n           \"type\": \"object\"\n          },\n          \"columnMaskHashV2\": {\n           \"additionalProperties\": {\n            \"additionalProperties\": {\n             \"additionalProperties\": {\n              \"description\": \"The list of regular expressions that match the fully-qualified names of character-based columns (e.g. inventory.orders.customerName)\",\n              \"items\": {\n               \"type\": \"string\"\n              },\n              \"type\": \"array\"\n             },\n             \"description\": \"The salt (e.g. CzQMA0cB5K) value and configuration.\",\n             \"type\": \"object\"\n            },\n            \"description\": \"The hash algorithm (e.g. SHA-256) type and configuration.\",\n            \"type\": \"object\"\n           },\n           \"description\": \"Similar to also columnMaskHash but using hashing strategy version 2.\\n Hashing strategy version 2 should be used to ensure fidelity if the value is being hashed in different places or systems.\\n\",\n           \"type\": \"object\"\n          },\n          \"columnMaskWithLengthChars\": {\n           \"description\": \"An optional, list of regular expressions that match the fully-qualified names of character-based columns. Set this property if you want the connector to mask the values for a set of columns, for example, if they contain sensitive data. Set length to a positive integer to replace data in the specified columns with the number of asterisk (*) characters specified by the length in the property name. Set length to 0 (zero) to replace data in the specified columns with an empty string.\\n The fully-qualified name of a column observes the following format: schemaName.tableName.columnName.\\n To match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; the expression does not match substrings that might be present in a column name.\\n You can specify multiple properties with different lengths in a single configuration.\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"columnPropagateSourceType\": {\n           \"description\": \"Default `[.*]`. An optional, list of regular expressions that match the fully-qualified names of columns for which you want the connector to emit extra parameters that represent column metadata. When this property is set, the connector adds the following fields to the schema of event records:\\n\\n* `__debezium.source.column.type`\\n* `__debezium.source.column.length`\\n* `__debezium.source.column.scale`\\n\\nThese parameters propagate a columnâ€™s original type name and length (for variable-width types), respectively.\\n Enabling the connector to emit this extra data can assist in properly sizing specific numeric or character-based columns in sink databases.\\n The fully-qualified name of a column observes one of the following formats: databaseName.tableName.columnName, or databaseName.schemaName.tableName.columnName.\\n To match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; the expression does not match substrings that might be present in a column name.\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"columnTruncateToLengthChars\": {\n           \"description\": \"An optional, list of regular expressions that match the fully-qualified names of character-based columns. Set this property if you want to truncate the data in a set of columns when it exceeds the number of characters specified by the length in the property name. Set length to a positive integer value, for example, column.truncate.to.20.chars.\\n The fully-qualified name of a column observes the following format: \\u003cschemaName\\u003e.\\u003ctableName\\u003e.\\u003ccolumnName\\u003e.\\n To match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; the expression does not match substrings that might be present in a column name.\\n You can specify multiple properties with different lengths in a single configuration.\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"converters\": {\n           \"additionalProperties\": {\n            \"additionalProperties\": {\n             \"type\": \"string\"\n            },\n            \"type\": \"object\"\n           },\n           \"description\": \"Enumerates a comma-separated list of the symbolic names of the [custom converter](https://debezium.io/documentation/reference/stable/development/converters.html#custom-converters) instances that the connector can use. For example,\\n\\n```\\nisbn:\\n  type: io.debezium.test.IsbnConverter\\n  schemaName: io.debezium.postgresql.type.Isbn\\n```\\n\\nYou must set the converters property to enable the connector to use a custom converter.\\n For each converter that you configure for a connector, you must also add a .type property, which specifies the fully-qualified name of the class that implements the converter interface.\\nIf you want to further control the behavior of a configured converter, you can add one or more configuration parameters to pass values to the converter. To associate any additional configuration parameter with a converter, prefix the parameter names with the symbolic name of the converter.\\n Each property is converted from myPropertyName to my.property.name\\n\",\n           \"type\": \"object\"\n          },\n          \"customMetricTags\": {\n           \"additionalProperties\": {\n            \"type\": \"string\"\n           },\n           \"description\": \"The custom metric tags will accept key-value pairs to customize the MBean object name which should be appended the end of regular name, each key would represent a tag for the MBean object name, and the corresponding value would be the value of that tag the key is. For example:\\n\\n```\\ncustomMetricTags:\\n  k1: v1\\n  k2: v2\\n```\\n\",\n           \"type\": \"object\"\n          },\n          \"databaseInitialStatements\": {\n           \"description\": \"A list of SQL statements that the connector executes when it establishes a JDBC connection to the database.\\n The connector may establish JDBC connections at its own discretion. Consequently, this property is useful for configuration of session parameters only, and not for executing DML statements.\\n The connector does not execute these statements when it creates a connection for reading the transaction log.\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"datatypePropagateSourceType\": {\n           \"description\": \"Default `[.*]`. An optional, list of regular expressions that specify the fully-qualified names of data types that are defined for columns in a database. When this property is set, for columns with matching data types, the connector emits event records that include the following extra fields in their schema:\\n\\n* `__debezium.source.column.type`\\n* `__debezium.source.column.length`\\n* `__debezium.source.column.scale`\\n\\nThese parameters propagate a columnâ€™s original type name and length (for variable-width types), respectively.\\n Enabling the connector to emit this extra data can assist in properly sizing specific numeric or character-based columns in sink databases.\\n The fully-qualified name of a column observes one of the following formats: databaseName.tableName.typeName, or databaseName.schemaName.tableName.typeName.\\n To match the name of a data type, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the data type; the expression does not match substrings that might be present in a type name.\\n For the list of PostgreSQL-specific data type names, see the [PostgreSQL data type mappings](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-data-types).\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"decimalHandlingMode\": {\n           \"description\": \"Default `precise`. Specifies how the connector should handle values for DECIMAL and NUMERIC columns:\\n\\n* `precise`: represents values by using java.math.BigDecimal to represent values in binary form in change events.\\n* `double`: represents values by using double values, which might result in a loss of precision but which is easier to use.\\n* `string`: encodes values as formatted strings, which are easy to consume but semantic information about the real type is lost. For more information, see [Decimal types](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-decimal-types).\\n\",\n           \"type\": \"string\"\n          },\n          \"errorsMaxRetries\": {\n           \"description\": \"Default `-1`. Specifies how the connector responds after an operation that results in a retriable error, such as a connection error.\\n\\nSet one of the following options:\\n\\n* `-1`: No limit. The connector always restarts automatically, and retries the operation, regardless of the number of previous failures.\\n* `0`: Disabled. The connector fails immediately, and never retries the operation. User intervention is required to restart the connector.\\n* `\\u003e 0`: The connector restarts automatically until it reaches the specified maximum number of retries. After the next failure, the connector stops, and user intervention is required to restart it.\\n\",\n           \"type\": \"integer\"\n          },\n          \"eventProcessingFailureHandlingMode\": {\n           \"description\": \"Default `fail`. Specifies how the connector should react to exceptions during processing of events:\\n\\n* `fail`: propagates the exception, indicates the offset of the problematic event, and causes the connector to stop.\\n* `warn`: logs the offset of the problematic event, skips that event, and continues processing.\\n* `skip`: skips the problematic event and continues processing.\\n\",\n           \"type\": \"string\"\n          },\n          \"fieldNameAdjustmentMode\": {\n           \"description\": \"Default `none`. Specifies how field names should be adjusted for compatibility with the message converter used by the connector. Possible settings:\\n\\n* `none` does not apply any adjustment.\\n* `avro` replaces the characters that cannot be used in the Avro type name with underscore.\\n* `avro_unicode` replaces the underscore or characters that cannot be used in the Avro type name with corresponding unicode like _uxxxx. Note: _ is an escape sequence like backslash in Java\\n\\nFor more information, see [Avro naming](https://debezium.io/documentation/reference/stable/configuration/avro.html#avro-naming).\\n\",\n           \"type\": \"string\"\n          },\n          \"flushLsnSource\": {\n           \"description\": \"Default `true`. Determines whether the connector should commit the LSN of the processed records in the source postgres database so that the WAL logs can be deleted. Specify false if you donâ€™t want the connector to do this. Please note that if set to false LSN will not be acknowledged by Debezium and as a result WAL logs will not be cleared which might result in disk space issues. User is expected to handle the acknowledgement of LSN outside Debezium.\\n\",\n           \"type\": \"boolean\"\n          },\n          \"heartbeatActionQuery\": {\n           \"description\": \"Specifies a query that the connector executes on the source database when the connector sends a heartbeat message.\\n This is useful for resolving the situation described in [WAL disk space consumption](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-wal-disk-space), where capturing changes from a low-traffic database on the same host as a high-traffic database prevents Debezium from processing WAL records and thus acknowledging WAL positions with the database. To address this situation, create a heartbeat table in the low-traffic database, and set this property to a statement that inserts records into that table, for example:\\n\\n ```\\n INSERT INTO test_heartbeat_table (text) VALUES ('test_heartbeat')\\n ```\\n \\n This allows the connector to receive changes from the low-traffic database and acknowledge their LSNs, which prevents unbounded WAL growth on the database host.\\n\",\n           \"type\": \"string\"\n          },\n          \"heartbeatIntervalMs\": {\n           \"description\": \"Default `0`. Controls how frequently the connector sends heartbeat messages to a Kafka topic. The default behavior is that the connector does not send heartbeat messages.\\n Heartbeat messages are useful for monitoring whether the connector is receiving change events from the database. Heartbeat messages might help decrease the number of change events that need to be re-sent when a connector restarts. To send heartbeat messages, set this property to a positive integer, which indicates the number of milliseconds between heartbeat messages.\\n Heartbeat messages are needed when there are many updates in a database that is being tracked but only a tiny number of updates are related to the table(s) and schema(s) for which the connector is capturing changes. In this situation, the connector reads from the database transaction log as usual but rarely emits change records to Kafka. This means that no offset updates are committed to Kafka and the connector does not have an opportunity to send the latest retrieved LSN to the database. The database retains WAL files that contain events that have already been processed by the connector. Sending heartbeat messages enables the connector to send the latest retrieved LSN to the database, which allows the database to reclaim disk space being used by no longer needed WAL files.\\n\",\n           \"type\": \"integer\"\n          },\n          \"hstoreHandlingMode\": {\n           \"description\": \"Default `json`. Specifies how the connector should handle values for hstore columns:\\n\\n* `map`: represents values by using MAP.\\n* `json`: represents values by using json string. This setting encodes values as formatted strings such as {\\\"key\\\" : \\\"val\\\"}. For more information, see [PostgreSQL HSTORE type](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-hstore-type).\\n\",\n           \"type\": \"string\"\n          },\n          \"includeUnknownDatatypes\": {\n           \"description\": \"Default `true`. Specifies connector behavior when the connector encounters a field whose data type is unknown. The default behavior is that the connector omits the field from the change event and logs a warning.\\n Set this property to true if you want the change event to contain an opaque binary representation of the field. This lets consumers decode the field. You can control the exact representation by setting the binaryHandlingMode property.\\n\\u003e *NOTE*: Consumers risk backward compatibility issues when `includeUnknownDatatypes` is set to `true`. Not only may the database-specific binary representation change between releases, but if the data type is eventually supported by Debezium, the data type will be sent downstream in a logical type, which would require adjustments by consumers. In general, when encountering unsupported data types, create a feature request so that support can be added.\\n\",\n           \"type\": \"boolean\"\n          },\n          \"incrementalSnapshotChunkSize\": {\n           \"description\": \"Default `1024`. The maximum number of rows that the connector fetches and reads into memory during an incremental snapshot chunk. Increasing the chunk size provides greater efficiency, because the snapshot runs fewer snapshot queries of a greater size. However, larger chunk sizes also require more memory to buffer the snapshot data. Adjust the chunk size to a value that provides the best performance in your environment.\\n\",\n           \"type\": \"integer\"\n          },\n          \"incrementalSnapshotWatermarkingStrategy\": {\n           \"description\": \"Default `insert_insert`. Specifies the watermarking mechanism that the connector uses during an incremental snapshot to deduplicate events that might be captured by an incremental snapshot and then recaptured after streaming resumes.\\n\\nYou can specify one of the following options:\\n\\n* `insert_insert`: When you send a signal to initiate an incremental snapshot, for every chunk that Debezium reads during the snapshot, it writes an entry to the signaling data collection to record the signal to open the snapshot window. After the snapshot completes, Debezium inserts a second entry to record the closing of the window.\\n* `insert_delete`: When you send a signal to initiate an incremental snapshot, for every chunk that Debezium reads, it writes a single entry to the signaling data collection to record the signal to open the snapshot window. After the snapshot completes, this entry is removed. No entry is created for the signal to close the snapshot window. Set this option to prevent rapid growth of the signaling data collection.\\n\",\n           \"type\": \"string\"\n          },\n          \"intervalHandlingMode\": {\n           \"description\": \"Default `numeric`. Specifies how the connector should handle values for interval columns:\\n\\n * `numeric`: represents intervals using approximate number of microseconds.\\n * `string`: represents intervals exactly by using the string pattern representation P\\u003cyears\\u003eY\\u003cmonths\\u003eM\\u003cdays\\u003eDT\\u003chours\\u003eH\\u003cminutes\\u003eM\\u003cseconds\\u003eS. For example: P1Y2M3DT4H5M6.78S. For more information, see [PostgreSQL basic types](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-basic-types).\\n\",\n           \"type\": \"string\"\n          },\n          \"maxBatchSize\": {\n           \"description\": \"Default `2048`. Positive integer value that specifies the maximum size of each batch of events that the connector processes.\\n\",\n           \"type\": \"integer\"\n          },\n          \"maxQueueSize\": {\n           \"description\": \"Default `8192`. Positive integer value that specifies the maximum number of records that the blocking queue can hold. When Debezium reads events streamed from the database, it places the events in the blocking queue before it writes them to Kafka. The blocking queue can provide backpressure for reading change events from the database in cases where the connector ingests messages faster than it can write them to Kafka, or when Kafka becomes unavailable. Events that are held in the queue are disregarded when the connector periodically records offsets. Always set the value of maxQueueSize to be larger than the value of maxBatchSize.\\n\",\n           \"type\": \"integer\"\n          },\n          \"maxQueueSizeInBytes\": {\n           \"description\": \"Default `0`. A long integer value that specifies the maximum volume of the blocking queue in bytes. By default, volume limits are not specified for the blocking queue. To specify the number of bytes that the queue can consume, set this property to a positive long value.\\n If maxQueueSize is also set, writing to the queue is blocked when the size of the queue reaches the limit specified by either property. For example, if you set maxQueueSize=1000, and maxQueueSizeInBytes=5000, writing to the queue is blocked after the queue contains 1000 records, or after the volume of the records in the queue reaches 5000 bytes.\\n\",\n           \"type\": \"integer\"\n          },\n          \"messageKeyColumns\": {\n           \"description\": \"A list of expressions that specify the columns that the connector uses to form custom message keys for change event records that it publishes to the Kafka topics for specified tables.\\n By default, Debezium uses the primary key column of a table as the message key for records that it emits. In place of the default, or to specify a key for tables that lack a primary key, you can configure custom message keys based on one or more columns.\\n To establish a custom message key for a table, list the table, followed by the columns to use as the message key. Each list entry takes the following format:\\n \\u003cfully-qualified_tableName\\u003e:\\u003ckeyColumn\\u003e,\\u003ckeyColumn\\u003e\\n To base a table key on multiple column names, insert commas between the column names.\\n Each fully-qualified table name is a regular expression in the following format:\\n \\u003cschemaName\\u003e.\\u003ctableName\\u003e\\n The property can include entries for multiple tables. Use a semicolon to separate table entries in the list.\\n The following example sets the message key for the tables inventory.customers and purchase.orders:\\n inventory.customers:pk1,pk2;(.*).purchaseorders:pk3,pk4\\n For the table inventory.customer, the columns pk1 and pk2 are specified as the message key. For the purchaseorders tables in any schema, the columns pk3 and pk4 server as the message key.\\n There is no limit to the number of columns that you use to create custom message keys. However, itâ€™s best to use the minimum number that are required to specify a unique key.\\n Note that having this property set and REPLICA IDENTITY set to DEFAULT on the tables, will cause the tombstone events to not be created properly if the key columns are not part of the primary key of the table. Setting REPLICA IDENTITY to FULL is the only solution.\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"moneyFractionDigits\": {\n           \"description\": \"Default `2`. Specifies how many decimal digits should be used when converting Postgres money type to java.math.BigDecimal, which represents the values in change events. Applicable only when decimalHandlingMode is set to precise.\\n\",\n           \"type\": \"integer\"\n          },\n          \"notificationEnabledChannels\": {\n           \"description\": \"List of notification channel names that are enabled for the connector. By default, the following channels are available: sink, log and jmx. Optionally, you can also implement a [custom notification channel](https://debezium.io/documentation/reference/stable/configuration/signalling.html#debezium-signaling-enabling-custom-signaling-channel).\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"pluginName\": {\n           \"description\": \"Default `pgoutput`. The name of the [PostgreSQL logical decoding plug-in](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-output-plugin) installed on the PostgreSQL server. Supported values are decoderbufs, and pgoutput.\\n\",\n           \"type\": \"string\"\n          },\n          \"pollIntervalMs\": {\n           \"description\": \"Default `500`. Positive integer value that specifies the number of milliseconds the connector should wait for new change events to appear before it starts processing a batch of events. Defaults to 500 milliseconds.\\n\",\n           \"type\": \"integer\"\n          },\n          \"provideTransactionMetadata\": {\n           \"description\": \"Default `false`. Determines whether the connector generates events with transaction boundaries and enriches change event envelopes with transaction metadata. Specify true if you want the connector to do this. For more information, see [Transaction metadata](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-transaction-metadata).\\n\",\n           \"type\": \"boolean\"\n          },\n          \"publicationAutocreateMode\": {\n           \"description\": \"Default `all_tables`. Applies only when streaming changes by using [the pgoutput plug-in](https://www.postgresql.org/docs/current/sql-createpublication.html). The setting determines how creation of a [publication](https://www.postgresql.org/docs/current/logical-replication-publication.html) should work. Specify one of the following values:\\n\\n* `all_tables` - If a publication exists, the connector uses it. If a publication does not exist, the connector creates a publication for all tables in the database for which the connector is capturing changes. For the connector to create a publication it must access the database through a database user account that has permission to create publications and perform replications. You grant the required permission by using the following SQL command CREATE PUBLICATION \\u003cpublication_name\\u003e FOR ALL TABLES;.\\n* `disabled` - The connector does not attempt to create a publication. A database administrator or the user configured to perform replications must have created the publication before running the connector. If the connector cannot find the publication, the connector throws an exception and stops.\\n* `filtered` - If a publication exists, the connector uses it. If no publication exists, the connector creates a new publication for tables that match the current filter configuration as specified by the schema.include.list, schema.exclude.list, and table.include.list, and table.exclude.list connector configuration properties. For example: CREATE PUBLICATION \\u003cpublication_name\\u003e FOR TABLE \\u003ctbl1, tbl2, tbl3\\u003e. If the publication exists, the connector updates the publication for tables that match the current filter configuration. For example: ALTER PUBLICATION \\u003cpublication_name\\u003e SET TABLE \\u003ctbl1, tbl2, tbl3\\u003e.\\n\",\n           \"type\": \"string\"\n          },\n          \"publicationName\": {\n           \"description\": \"Default \\u003cSGStream name\\u003e.\\u003cSGStream namespace\\u003e (with all characters that are not `[a-zA-Z0-9]` changed to `_` character). The name of the PostgreSQL publication created for streaming changes when using pgoutput. This publication is created at start-up if it does not already exist and it includes all tables. Debezium then applies its own include/exclude list filtering, if configured, to limit the publication to change events for the specific tables of interest. The connector user must have superuser permissions to create this publication, so it is usually preferable to create the publication before starting the connector for the first time. If the publication already exists, either for all tables or configured with a subset of tables, Debezium uses the publication as it is defined.\\n\",\n           \"type\": \"string\"\n          },\n          \"replicaIdentityAutosetValues\": {\n           \"description\": \"The setting determines the value for [replica identity](https://www.postgresql.org/docs/current/sql-altertable.html#SQL-ALTERTABLE-REPLICA-IDENTITY) at table level.\\n  This option will overwrite the existing value in database. A comma-separated list of regular expressions that match fully-qualified tables and replica identity value to be used in the table.\\n  Each expression must match the pattern '\\u003cfully-qualified table name\\u003e:\\u003creplica identity\\u003e', where the table name could be defined as (SCHEMA_NAME.TABLE_NAME), and the replica identity values are:\\n  DEFAULT - Records the old values of the columns of the primary key, if any. This is the default for non-system tables.\\n  INDEX index_name - Records the old values of the columns covered by the named index, that must be unique, not partial, not deferrable, and include only columns marked NOT NULL. If this index is dropped, the behavior is the same as NOTHING.\\n  FULL - Records the old values of all columns in the row.\\n  NOTHING - Records no information about the old row. This is the default for system tables.\\n  For example,\\n  schema1.*:FULL,schema2.table2:NOTHING,schema2.table3:INDEX idx_name\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"retriableRestartConnectorWaitMs\": {\n           \"description\": \"Default `10000` (10 seconds). The number of milliseconds to wait before restarting a connector after a retriable error occurs.\\n\",\n           \"type\": \"integer\"\n          },\n          \"schemaNameAdjustmentMode\": {\n           \"description\": \"Default `none`. Specifies how schema names should be adjusted for compatibility with the message converter used by the connector. Possible settings:\\n\\n* `none` does not apply any adjustment.\\n* `avro` replaces the characters that cannot be used in the Avro type name with underscore.\\n* `avro_unicode` replaces the underscore or characters that cannot be used in the Avro type name with corresponding unicode like _uxxxx. Note: _ is an escape sequence like backslash in Java\\n\",\n           \"type\": \"string\"\n          },\n          \"schemaRefreshMode\": {\n           \"description\": \"Default `columns_diff`. Specify the conditions that trigger a refresh of the in-memory schema for a table.\\n\\n* `columns_diff`: is the safest mode. It ensures that the in-memory schema stays in sync with the database tableâ€™s schema at all times.\\n* `columns_diff_exclude_unchanged_toast`: instructs the connector to refresh the in-memory schema cache if there is a discrepancy with the schema derived from the incoming message, unless unchanged TOASTable data fully accounts for the discrepancy.\\n\\nThis setting can significantly improve connector performance if there are frequently-updated tables that have TOASTed data that are rarely part of updates. However, it is possible for the in-memory schema to become outdated if TOASTable columns are dropped from the table.\\n\",\n           \"type\": \"string\"\n          },\n          \"signalDataCollection\": {\n           \"description\": \"Fully-qualified name of the data collection that is used to send signals to the connector. Use the following format to specify the collection name: \\u003cschemaName\\u003e.\\u003ctableName\\u003e\\n\",\n           \"type\": \"string\"\n          },\n          \"signalEnabledChannels\": {\n           \"description\": \"Default `[sgstream-annotations]`. List of the signaling channel names that are enabled for the connector. By default, the following channels are available: sgstream-annotations, source, kafka, file and jmx. Optionally, you can also implement a [custom signaling channel](https://debezium.io/documentation/reference/stable/configuration/signalling.html#debezium-signaling-enabling-custom-signaling-channel).\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"skipMessagesWithoutChange\": {\n           \"description\": \"Default `false`. Specifies whether to skip publishing messages when there is no change in included columns. This would essentially filter messages if there is no change in columns included as per includes or excludes fields. Note: Only works when REPLICA IDENTITY of the table is set to FULL\\n\",\n           \"type\": \"boolean\"\n          },\n          \"skippedOperations\": {\n           \"description\": \"Default `none`. A list of operation types that will be skipped during streaming. The operations include: c for inserts/create, u for updates, d for deletes, t for truncates, and none to not skip any operations. By default, no operations are skipped.\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"slotDropOnStop\": {\n           \"description\": \"Default `true`. Whether or not to delete the logical replication slot when the connector stops in a graceful, expected way. The default behavior is that the replication slot remains configured for the connector when the connector stops. When the connector restarts, having the same replication slot enables the connector to start processing where it left off. Set to true in only testing or development environments. Dropping the slot allows the database to discard WAL segments. When the connector restarts it performs a new snapshot or it can continue from a persistent offset in the Kafka Connect offsets topic.\\n\",\n           \"type\": \"boolean\"\n          },\n          \"slotMaxRetries\": {\n           \"description\": \"Default `6`. If connecting to a replication slot fails, this is the maximum number of consecutive attempts to connect.\\n\",\n           \"type\": \"integer\"\n          },\n          \"slotName\": {\n           \"description\": \"Default \\u003cSGStream namespace\\u003e.\\u003cSGStream name\\u003e (with all characters that are not `[a-zA-Z0-9]` changed to `_` character). The name of the PostgreSQL logical decoding slot that was created for streaming changes from a particular plug-in for a particular database/schema. The server uses this slot to stream events to the Debezium connector that you are configuring.\\n\\nSlot names must conform to [PostgreSQL replication slot naming rules](https://www.postgresql.org/docs/current/static/warm-standby.html#STREAMING-REPLICATION-SLOTS-MANIPULATION), which state: \\\"Each replication slot has a name, which can contain lower-case letters, numbers, and the underscore character.\\\"\\n\",\n           \"type\": \"string\"\n          },\n          \"slotRetryDelayMs\": {\n           \"description\": \"Default `10000` (10 seconds). The number of milliseconds to wait between retry attempts when the connector fails to connect to a replication slot.\\n\",\n           \"type\": \"integer\"\n          },\n          \"slotStreamParams\": {\n           \"additionalProperties\": {\n            \"type\": \"string\"\n           },\n           \"description\": \"Parameters to pass to the configured logical decoding plug-in. For example:\\n\\n```\\nslotStreamParams:\\n  add-tables: \\\"public.table,public.table2\\\"\\n  include-lsn: \\\"true\\\"\\n```\\n\",\n           \"type\": \"object\"\n          },\n          \"snapshotDelayMs\": {\n           \"description\": \"An interval in milliseconds that the connector should wait before performing a snapshot when the connector starts. If you are starting multiple connectors in a cluster, this property is useful for avoiding snapshot interruptions, which might cause re-balancing of connectors.\\n\",\n           \"type\": \"integer\"\n          },\n          \"snapshotFetchSize\": {\n           \"description\": \"Default `10240`. During a snapshot, the connector reads table content in batches of rows. This property specifies the maximum number of rows in a batch.\\n\",\n           \"type\": \"integer\"\n          },\n          \"snapshotIncludeCollectionList\": {\n           \"description\": \"Default \\u003cAll tables / All tables filtered in `includes` field / All tables that are not filtered out in `excludes` field\\u003e. An optional, list of regular expressions that match the fully-qualified names (\\u003cschemaName\\u003e.\\u003ctableName\\u003e) of the tables to include in a snapshot. The specified items must be named in the connectorâ€™s table.include.list property. This property takes effect only if the connectorâ€™s snapshotMode property is set to a value other than `never`. This property does not affect the behavior of incremental snapshots.\\n   To match the name of a table, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the table; it does not match substrings that might be present in a table name.\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"snapshotLockTimeoutMs\": {\n           \"description\": \"Default `10000`. Positive integer value that specifies the maximum amount of time (in milliseconds) to wait to obtain table locks when performing a snapshot. If the connector cannot acquire table locks in this time interval, the snapshot fails. [How the connector performs snapshots](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-snapshots) provides details.\\n\",\n           \"type\": \"integer\"\n          },\n          \"snapshotLockingMode\": {\n           \"description\": \"Default `none`. Specifies how the connector holds locks on tables while performing a schema snapshot. Set one of the following options:\\n\\n* `shared`: The connector holds a table lock that prevents exclusive table access during the initial portion phase of the snapshot in which database schemas and other metadata are read. After the initial phase, the snapshot no longer requires table locks.\\n* `none`: The connector avoids locks entirely. Do not use this mode if schema changes might occur during the snapshot.\\n\\n\\u003e *WARNING*: Do not use this mode if schema changes might occur during the snapshot.\\n\\n* `custom`: The connector performs a snapshot according to the implementation specified by the snapshotLockingModeCustomName property, which is a custom implementation of the io.debezium.spi.snapshot.SnapshotLock interface.\\n\",\n           \"type\": \"string\"\n          },\n          \"snapshotLockingModeCustomName\": {\n           \"description\": \"When snapshotLockingMode is set to custom, use this setting to specify the name of the custom implementation provided in the name() method that is defined by the 'io.debezium.spi.snapshot.SnapshotLock' interface. For more information, see [custom snapshotter SPI](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#connector-custom-snapshot).\\n\",\n           \"type\": \"string\"\n          },\n          \"snapshotMaxThreads\": {\n           \"description\": \"Default `1`. Specifies the number of threads that the connector uses when performing an initial snapshot. To enable parallel initial snapshots, set the property to a value greater than 1. In a parallel initial snapshot, the connector processes multiple tables concurrently. This feature is incubating.\\n\",\n           \"type\": \"integer\"\n          },\n          \"snapshotMode\": {\n           \"description\": \"Default `initial`. Specifies the criteria for performing a snapshot when the connector starts:\\n\\n* `always` - The connector performs a snapshot every time that it starts. The snapshot includes the structure and data of the captured tables. Specify this value to populate topics with a complete representation of the data from the captured tables every time that the connector starts. After the snapshot completes, the connector begins to stream event records for subsequent database changes.\\n* `initial` - The connector performs a snapshot only when no offsets have been recorded for the logical server name.\\n* `initial_only` - The connector performs an initial snapshot and then stops, without processing any subsequent changes.\\n* `no_data` - The connector never performs snapshots. When a connector is configured this way, after it starts, it behaves as follows: If there is a previously stored LSN in the Kafka offsets topic, the connector continues streaming changes from that position. If no LSN is stored, the connector starts streaming changes from the point in time when the PostgreSQL logical replication slot was created on the server. Use this snapshot mode only when you know all data of interest is still reflected in the WAL.\\n* `never` - Deprecated see no_data.\\n* `when_needed` - After the connector starts, it performs a snapshot only if it detects one of the following circumstances: \\n  It cannot detect any topic offsets.\\n  A previously recorded offset specifies a log position that is not available on the server.\\n* `configuration_based` - With this option, you control snapshot behavior through a set of connector properties that have the prefix 'snapshotModeConfigurationBased'.\\n* `custom` - The connector performs a snapshot according to the implementation specified by the snapshotModeCustomName property, which defines a custom implementation of the io.debezium.spi.snapshot.Snapshotter interface.\\n\\nFor more information, see the [table of snapshot.mode options](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-connector-snapshot-mode-options).\\n\",\n           \"type\": \"string\"\n          },\n          \"snapshotModeConfigurationBasedSnapshotData\": {\n           \"description\": \"Default `false`. If the snapshotMode is set to configuration_based, set this property to specify whether the connector includes table data when it performs a snapshot.\\n\",\n           \"type\": \"boolean\"\n          },\n          \"snapshotModeConfigurationBasedSnapshotOnDataError\": {\n           \"description\": \"Default `false`. If the snapshotMode is set to configuration_based, this property specifies whether the connector attempts to snapshot table data if it does not find the last committed offset in the transaction log. Set the value to true to instruct the connector to perform a new snapshot.\\n\",\n           \"type\": \"boolean\"\n          },\n          \"snapshotModeConfigurationBasedSnapshotOnSchemaError\": {\n           \"description\": \"Default `false`. If the snapshotMode is set to configuration_based, set this property to specify whether the connector includes table schema in a snapshot if the schema history topic is not available.\\n\",\n           \"type\": \"boolean\"\n          },\n          \"snapshotModeConfigurationBasedSnapshotSchema\": {\n           \"description\": \"Default `false`. If the snapshotMode is set to configuration_based, set this property to specify whether the connector includes the table schema when it performs a snapshot.\\n\",\n           \"type\": \"boolean\"\n          },\n          \"snapshotModeConfigurationBasedStartStream\": {\n           \"description\": \"Default `false`. If the snapshotMode is set to configuration_based, set this property to specify whether the connector begins to stream change events after a snapshot completes.\\n\",\n           \"type\": \"boolean\"\n          },\n          \"snapshotModeCustomName\": {\n           \"description\": \"When snapshotMode is set as custom, use this setting to specify the name of the custom implementation provided in the name() method that is defined by the 'io.debezium.spi.snapshot.Snapshotter' interface. The provided implementation is called after a connector restart to determine whether to perform a snapshot. For more information, see [custom snapshotter SPI](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#connector-custom-snapshot).\\n\",\n           \"type\": \"string\"\n          },\n          \"snapshotQueryMode\": {\n           \"description\": \"Default `select_all`. Specifies how the connector queries data while performing a snapshot. Set one of the following options:\\n\\n* `select_all`: The connector performs a select all query by default, optionally adjusting the columns selected based on the column include and exclude list configurations.\\n* `custom`: The connector performs a snapshot query according to the implementation specified by the snapshotQueryModeCustomName property, which defines a custom implementation of the io.debezium.spi.snapshot.SnapshotQuery interface. This setting enables you to manage snapshot content in a more flexible manner compared to using the snapshotSelectStatementOverrides property.\\n\",\n           \"type\": \"string\"\n          },\n          \"snapshotQueryModeCustomName\": {\n           \"description\": \"When snapshotQueryMode is set as custom, use this setting to specify the name of the custom implementation provided in the name() method that is defined by the 'io.debezium.spi.snapshot.SnapshotQuery' interface. For more information, see [custom snapshotter SPI](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#connector-custom-snapshot).\\n\",\n           \"type\": \"string\"\n          },\n          \"snapshotSelectStatementOverrides\": {\n           \"additionalProperties\": {\n            \"type\": \"string\"\n           },\n           \"description\": \"Specifies the table rows to include in a snapshot. Use the property if you want a snapshot to include only a subset of the rows in a table. This property affects snapshots only. It does not apply to events that the connector reads from the log.\\n The property contains a hierarchy of fully-qualified table names in the form \\u003cschemaName\\u003e.\\u003ctableName\\u003e. For example,\\n\\n```\\nsnapshotSelectStatementOverrides: \\n  \\\"customers.orders\\\": \\\"SELECT * FROM [customers].[orders] WHERE delete_flag = 0 ORDER BY id DESC\\\"\\n```\\n\\nIn the resulting snapshot, the connector includes only the records for which delete_flag = 0.\\n\",\n           \"type\": \"object\"\n          },\n          \"statusUpdateIntervalMs\": {\n           \"description\": \"Default `10000`. Frequency for sending replication connection status updates to the server, given in milliseconds. The property also controls how frequently the database status is checked to detect a dead connection in case the database was shut down.\\n\",\n           \"type\": \"integer\"\n          },\n          \"timePrecisionMode\": {\n           \"description\": \"Default `adaptive`. Time, date, and timestamps can be represented with different kinds of precision:\\n\\n* `adaptive`: captures the time and timestamp values exactly as in the database using either millisecond, microsecond, or nanosecond precision values based on the database columnâ€™s type.\\n* `adaptive_time_microseconds`: captures the date, datetime and timestamp values exactly as in the database using either millisecond, microsecond, or nanosecond precision values based on the database columnâ€™s type. An exception is TIME type fields, which are always captured as microseconds.\\n* `connect`: always represents time and timestamp values by using Kafka Connectâ€™s built-in representations for Time, Date, and Timestamp, which use millisecond precision regardless of the database columns' precision. For more information, see [temporal values](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-temporal-types).\\n\",\n           \"type\": \"string\"\n          },\n          \"tombstonesOnDelete\": {\n           \"description\": \"Default `true`. Controls whether a delete event is followed by a tombstone event.\\n\\n* `true` - a delete operation is represented by a delete event and a subsequent tombstone event.\\n* `false` - only a delete event is emitted.\\n\\nAfter a source record is deleted, emitting a tombstone event (the default behavior) allows Kafka to completely delete all events that pertain to the key of the deleted row in case [log compaction](https://kafka.apache.org/documentation/#compaction) is enabled for the topic.\\n\",\n           \"type\": \"boolean\"\n          },\n          \"topicCacheSize\": {\n           \"description\": \"Default `10000`. The size used for holding the topic names in bounded concurrent hash map. This cache will help to determine the topic name corresponding to a given data collection.\\n\",\n           \"type\": \"integer\"\n          },\n          \"topicDelimiter\": {\n           \"description\": \"Default `.`. Specify the delimiter for topic name, defaults to \\\".\\\".\\n\",\n           \"type\": \"string\"\n          },\n          \"topicHeartbeatPrefix\": {\n           \"description\": \"Default `__debezium-heartbeat`. Controls the name of the topic to which the connector sends heartbeat messages. For example, if the topic prefix is fulfillment, the default topic name is __debezium-heartbeat.fulfillment.\\n\",\n           \"type\": \"string\"\n          },\n          \"topicNamingStrategy\": {\n           \"description\": \"Default `io.debezium.schema.SchemaTopicNamingStrategy`. The name of the TopicNamingStrategy class that should be used to determine the topic name for data change, schema change, transaction, heartbeat event etc., defaults to SchemaTopicNamingStrategy.\\n\",\n           \"type\": \"string\"\n          },\n          \"topicTransaction\": {\n           \"description\": \"Default `transaction`. Controls the name of the topic to which the connector sends transaction metadata messages. For example, if the topic prefix is fulfillment, the default topic name is fulfillment.transaction.\\n\",\n           \"type\": \"string\"\n          },\n          \"unavailableValuePlaceholder\": {\n           \"description\": \"Default `__debezium_unavailable_value`. Specifies the constant that the connector provides to indicate that the original value is a toasted value that is not provided by the database. If the setting of unavailable.value.placeholder starts with the hex: prefix it is expected that the rest of the string represents hexadecimally encoded octets. For more information, see [toasted values](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-toasted-values).\\n\",\n           \"type\": \"string\"\n          },\n          \"xminFetchIntervalMs\": {\n           \"description\": \"Default `0`. How often, in milliseconds, the XMIN will be read from the replication slot. The XMIN value provides the lower bounds of where a new replication slot could start from. The default value of 0 disables tracking XMIN tracking.\\n\",\n           \"type\": \"integer\"\n          }\n         },\n         \"type\": \"object\"\n        },\n        \"excludes\": {\n         \"description\": \"A list of regular expressions that allow to match one or more `\\u003cschema\\u003e.\\u003ctable\\u003e.\\u003ccolumn\\u003e` entries to be filtered out before sending to the target.\\n\\nThis property is mutually exclusive with `includes`.\\n\",\n         \"items\": {\n          \"description\": \"A regular expressions that allow to match one or more `\\u003cschema\\u003e.\\u003ctable\\u003e.\\u003ccolumn\\u003e` entries to be filtered out before sending to the target.\\n\",\n          \"type\": \"string\"\n         },\n         \"type\": \"array\"\n        },\n        \"includes\": {\n         \"description\": \"A list of regular expressions that allow to match one or more `\\u003cschema\\u003e.\\u003ctable\\u003e.\\u003ccolumn\\u003e` entries to be filtered before sending to the target.\\n\\nThis property is mutually exclusive with `excludes`.\\n\",\n         \"items\": {\n          \"description\": \"A regular expressions that allow to match one or more `\\u003cschema\\u003e.\\u003ctable\\u003e.\\u003ccolumn\\u003e` entries to be filtered before sending to the target.\\n\",\n          \"type\": \"string\"\n         },\n         \"type\": \"array\"\n        },\n        \"name\": {\n         \"description\": \"The target SGCluster name.\\n\",\n         \"type\": \"string\"\n        },\n        \"password\": {\n         \"description\": \"The password used by the CDC process to connect to the database.\\n\\nIf not specified the default superuser password will be used.\\n\",\n         \"properties\": {\n          \"key\": {\n           \"description\": \"The Secret key where the password is stored.\\n\",\n           \"type\": \"string\"\n          },\n          \"name\": {\n           \"description\": \"The Secret name where the password is stored.\\n\",\n           \"type\": \"string\"\n          }\n         },\n         \"required\": [\n          \"name\",\n          \"key\"\n         ],\n         \"type\": \"object\"\n        },\n        \"username\": {\n         \"description\": \"The username used by the CDC process to connect to the database.\\n\\nIf not specified the default superuser username (by default postgres) will be used.\\n\",\n         \"properties\": {\n          \"key\": {\n           \"description\": \"The Secret key where the username is stored.\\n\",\n           \"type\": \"string\"\n          },\n          \"name\": {\n           \"description\": \"The Secret name where the username is stored.\\n\",\n           \"type\": \"string\"\n          }\n         },\n         \"required\": [\n          \"name\",\n          \"key\"\n         ],\n         \"type\": \"object\"\n        }\n       },\n       \"required\": [\n        \"name\"\n       ],\n       \"type\": \"object\"\n      },\n      \"type\": {\n       \"description\": \"The type of data source. Available data source types are:\\n\\n* `SGCluster`: an SGCluster in the same namespace\\n* `Postgres`: any Postgres instance\\n\",\n       \"type\": \"string\"\n      }\n     },\n     \"required\": [\n      \"type\"\n     ],\n     \"type\": \"object\"\n    },\n    \"target\": {\n     \"description\": \"The target of this sream.\\n\",\n     \"properties\": {\n      \"cloudEvent\": {\n       \"description\": \"Configuration section for `CloudEvent` target type.\\n\",\n       \"properties\": {\n        \"binding\": {\n         \"description\": \"The CloudEvent binding (http by default).\\n\\nOnly http is supported at the moment.\\n\",\n         \"type\": \"string\"\n        },\n        \"format\": {\n         \"description\": \"The CloudEvent format (json by default).\\n\\nOnly json is supported at the moment.\\n\",\n         \"type\": \"string\"\n        },\n        \"http\": {\n         \"description\": \"The http binding configuration.\",\n         \"properties\": {\n          \"connectTimeout\": {\n           \"description\": \"Set the connect timeout.\\n\\nValue 0 represents infinity (default). Negative values are not allowed.\\n\",\n           \"type\": \"string\"\n          },\n          \"headers\": {\n           \"additionalProperties\": {\n            \"type\": \"string\"\n           },\n           \"description\": \"Headers to include when sending CloudEvents to the endpoint.\",\n           \"type\": \"object\"\n          },\n          \"readTimeout\": {\n           \"description\": \"Set the read timeout. The value is the timeout to read a response.\\n\\nValue 0 represents infinity (default). Negative values are not allowed.\\n\",\n           \"type\": \"string\"\n          },\n          \"retryBackoffDelay\": {\n           \"default\": 60,\n           \"description\": \"The maximum amount of delay in seconds after an error before retrying again.\\n\\nThe initial delay will use 10% of this value and then increase the value exponentially up to the maximum amount of seconds specified with this field.\\n\",\n           \"type\": \"integer\"\n          },\n          \"retryLimit\": {\n           \"description\": \"Set the retry limit. When set the event will be sent again after an error for the specified limit of times. When not set the event will be sent again after an error.\\n\",\n           \"type\": \"integer\"\n          },\n          \"skipHostnameVerification\": {\n           \"description\": \"When `true` disable hostname verification.\",\n           \"type\": \"boolean\"\n          },\n          \"url\": {\n           \"description\": \"The URL used to send the CloudEvents to the endpoint.\",\n           \"type\": \"string\"\n          }\n         },\n         \"required\": [\n          \"url\"\n         ],\n         \"type\": \"object\"\n        }\n       },\n       \"type\": \"object\"\n      },\n      \"pgLambda\": {\n       \"description\": \"Configuration section for `PgLambda` target type.\\n\",\n       \"properties\": {\n        \"knative\": {\n         \"description\": \"Knative Service configuration.\",\n         \"properties\": {\n          \"annotations\": {\n           \"additionalProperties\": {\n            \"type\": \"string\"\n           },\n           \"description\": \"Annotations to set to Knative Service\",\n           \"type\": \"object\"\n          },\n          \"http\": {\n           \"description\": \"PgLambda uses a CloudEvent http binding to send events to the Knative Service. This section allow to modify the configuration of this binding.\",\n           \"properties\": {\n            \"connectTimeout\": {\n             \"description\": \"Set the connect timeout.\\n\\nValue 0 represents infinity (default). Negative values are not allowed.\\n\",\n             \"type\": \"string\"\n            },\n            \"headers\": {\n             \"additionalProperties\": {\n              \"type\": \"string\"\n             },\n             \"description\": \"Headers to include when sending CloudEvents to the endpoint.\",\n             \"type\": \"object\"\n            },\n            \"readTimeout\": {\n             \"description\": \"Set the read timeout. The value is the timeout to read a response.\\n\\nValue 0 represents infinity (default). Negative values are not allowed.\\n\",\n             \"type\": \"string\"\n            },\n            \"retryBackoffDelay\": {\n             \"default\": 60,\n             \"description\": \"The maximum amount of delay in seconds after an error before retrying again.\\n\\nThe initial delay will use 10% of this value and then increase the value exponentially up to the maximum amount of seconds specified with this field.\\n\",\n             \"type\": \"integer\"\n            },\n            \"retryLimit\": {\n             \"description\": \"Set the retry limit. When set the event will be sent again after an error for the specified limit of times. When not set the event will be sent again after an error.\\n\",\n             \"type\": \"integer\"\n            },\n            \"skipHostnameVerification\": {\n             \"description\": \"When `true` disable hostname verification.\",\n             \"type\": \"boolean\"\n            },\n            \"url\": {\n             \"description\": \"The URL used to send the CloudEvents to the endpoint.\",\n             \"type\": \"string\"\n            }\n           },\n           \"type\": \"object\"\n          },\n          \"labels\": {\n           \"additionalProperties\": {\n            \"type\": \"string\"\n           },\n           \"description\": \"Labels to set to Knative Service\",\n           \"type\": \"object\"\n          }\n         },\n         \"type\": \"object\"\n        },\n        \"script\": {\n         \"description\": \"Script to execute. This field is mutually exclusive with `scriptFrom` field.\\n\",\n         \"type\": \"string\"\n        },\n        \"scriptFrom\": {\n         \"description\": \"Reference to either a Kubernetes [Secret](https://kubernetes.io/docs/concepts/configuration/secret/) or a [ConfigMap](https://kubernetes.io/docs/concepts/configuration/configmap/) that contains the script to execute. This field is mutually exclusive with `script` field.\\n\\nFields `secretKeyRef` and `configMapKeyRef` are mutually exclusive, and one of them is required.\\n\",\n         \"properties\": {\n          \"configMapKeyRef\": {\n           \"description\": \"A [ConfigMap](https://kubernetes.io/docs/concepts/configuration/configmap/) reference that contains the script to execute. This field is mutually exclusive with `secretKeyRef` field.\\n\",\n           \"properties\": {\n            \"key\": {\n             \"description\": \"The key name within the ConfigMap that contains the script to execute.\\n\",\n             \"type\": \"string\"\n            },\n            \"name\": {\n             \"description\": \"The name of the ConfigMap that contains the script to execute.\\n\",\n             \"type\": \"string\"\n            }\n           },\n           \"type\": \"object\"\n          },\n          \"secretKeyRef\": {\n           \"description\": \"A Kubernetes [SecretKeySelector](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#secretkeyselector-v1-core) that contains the script to execute. This field is mutually exclusive with `configMapKeyRef` field.\\n\",\n           \"properties\": {\n            \"key\": {\n             \"description\": \"The key of the secret to select from. Must be a valid secret key.\",\n             \"type\": \"string\"\n            },\n            \"name\": {\n             \"description\": \"Name of the referent. [More information](https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names).\",\n             \"type\": \"string\"\n            }\n           },\n           \"type\": \"object\"\n          }\n         },\n         \"type\": \"object\"\n        },\n        \"scriptType\": {\n         \"description\": \"The PgLambda script format (javascript by default).\\n\\n* `javascript`: the script will receive the following variable:\\n  * `request`: the HTTP request object. See https://nodejs.org/docs/latest-v18.x/api/http.html#class-httpclientrequest\\n  * `response`: the HTTP response object. See https://nodejs.org/docs/latest-v18.x/api/http.html#class-httpserverresponse\\n  * `event`: the CloudEvent event object. See https://github.com/cloudevents/sdk-javascript\\n\",\n         \"type\": \"string\"\n        }\n       },\n       \"type\": \"object\"\n      },\n      \"sgCluster\": {\n       \"description\": \"The configuration of the data target required when type is `SGCluster`.\\n\",\n       \"properties\": {\n        \"database\": {\n         \"description\": \"The target database name to which the data will be migrated to.\\n\\nIf not specified the default postgres database will be targeted.\\n\",\n         \"type\": \"string\"\n        },\n        \"ddlImportRoleSkipFilter\": {\n         \"description\": \"Allow to set a [SIMILAR TO regular expression](https://www.postgresql.org/docs/current/functions-matching.html#FUNCTIONS-SIMILARTO-REGEXP) to match the names of the roles to skip during import of DDL.\\n\\nWhen not set and source is an SGCluster will match the superuser, replicator and authenticator usernames.\\n\",\n         \"type\": \"string\"\n        },\n        \"debeziumProperties\": {\n         \"description\": \"Specific property of the debezium JDBC sink.\\n\\nSee https://debezium.io/documentation/reference/stable/connectors/jdbc.html#jdbc-connector-configuration\\n\\nEach property is converted from myPropertyName to my.property.name\\n\",\n         \"properties\": {\n          \"batchSize\": {\n           \"description\": \"Default `500`. Specifies how many records to attempt to batch together into the destination table.\\n\\u003e Note that if you set `consumerMaxPollRecords` in the Connect worker properties to a value lower than `batchSize`, batch processing will be caped by `consumerMaxPollRecords` and the desired `batchSize` wonâ€™t be reached. You can also configure the connectorâ€™s underlying consumerâ€™s `maxPollRecords` using `consumerOverrideMaxPollRecords` in the connector configuration.\\n\",\n           \"type\": \"integer\"\n          },\n          \"columnNamingStrategy\": {\n           \"description\": \"Default `io.debezium.connector.jdbc.naming.DefaultColumnNamingStrategy`. Specifies the fully-qualified class name of a ColumnNamingStrategy implementation that the connector uses to resolve column names from event field names.\\nBy default, the connector uses the field name as the column name.\\n\",\n           \"type\": \"string\"\n          },\n          \"connectionPoolAcquire_increment\": {\n           \"description\": \"Default `32`. Specifies the number of connections that the connector attempts to acquire if the connection pool exceeds its maximum size.\\n\",\n           \"type\": \"integer\"\n          },\n          \"connectionPoolMax_size\": {\n           \"description\": \"Default `32`. Specifies the maximum number of concurrent connections that the pool maintains.\\n\",\n           \"type\": \"integer\"\n          },\n          \"connectionPoolMin_size\": {\n           \"description\": \"Default `5`. Specifies the minimum number of connections in the pool.\\n\",\n           \"type\": \"integer\"\n          },\n          \"connectionPoolTimeout\": {\n           \"description\": \"Default `1800`. Specifies the number of seconds that an unused connection is kept before it is discarded.\\n\",\n           \"type\": \"integer\"\n          },\n          \"databaseTime_zone\": {\n           \"description\": \"Default `UTC`. Specifies the timezone used when inserting JDBC temporal values.\\n\",\n           \"type\": \"string\"\n          },\n          \"deleteEnabled\": {\n           \"description\": \"Default `true`. Specifies whether the connector processes DELETE or tombstone events and removes the corresponding row from the database. Use of this option requires that you set the `primaryKeyMode` to `record_key`.\\n\",\n           \"type\": \"boolean\"\n          },\n          \"dialectPostgresPostgisSchema\": {\n           \"description\": \"Default `public`. Specifies the schema name where the PostgreSQL PostGIS extension is installed. The default is `public`; however, if the PostGIS extension was installed in another schema, this property should be used to specify the alternate schema name.\\n\",\n           \"type\": \"string\"\n          },\n          \"dialectSqlserverIdentityInsert\": {\n           \"description\": \"Default `false`. Specifies whether the connector automatically sets an IDENTITY_INSERT before an INSERT or UPSERT operation into the identity column of SQL Server tables, and then unsets it immediately after the operation. When the default setting (`false`) is in effect, an INSERT or UPSERT operation into the IDENTITY column of a table results in a SQL exception.\\n\",\n           \"type\": \"boolean\"\n          },\n          \"insertMode\": {\n           \"description\": \"Default `upsert`. Specifies the strategy used to insert events into the database. The following options are available:\\n* `insert`: Specifies that all events should construct INSERT-based SQL statements. Use this option only when no primary key is used, or when you can be certain that no updates can occur to rows with existing primary key values.\\n* `update`: Specifies that all events should construct UPDATE-based SQL statements. Use this option only when you can be certain that the connector receives only events that apply to existing rows.\\n* `upsert`: Specifies that the connector adds events to the table using upsert semantics. That is, if the primary key does not exist, the connector performs an INSERT operation, and if the key does exist, the connector performs an UPDATE operation. When idempotent writes are required, the connector should be configured to use this option.\\n\",\n           \"type\": \"string\"\n          },\n          \"primaryKeyFields\": {\n           \"description\": \"Either the name of the primary key column or a comma-separated list of fields to derive the primary key from.\\nWhen `primaryKeyMode` is set to `record_key` and the eventâ€™s key is a primitive type, it is expected that this property specifies the column name to be used for the key.\\nWhen the `primaryKeyMode` is set to `record_key` with a non-primitive key, or record_value, it is expected that this property specifies a comma-separated list of field names from either the key or value. If the primary.key.mode is set to record_key with a non-primitive key, or record_value, and this property is not specifies, the connector derives the primary key from all fields of either the record key or record value, depending on the specified mode.\\n\",\n           \"items\": {\n            \"type\": \"string\"\n           },\n           \"type\": \"array\"\n          },\n          \"primaryKeyMode\": {\n           \"description\": \"Default `record_key`. Specifies how the connector resolves the primary key columns from the event.\\n* `none`: Specifies that no primary key columns are created.\\n* `record_key`: Specifies that the primary key columns are sourced from the eventâ€™s record key. If the record key is a primitive type, the `primaryKeyFields` property is required to specify the name of the primary key column. If the record key is a struct type, the `primaryKeyFields` property is optional, and can be used to specify a subset of columns from the eventâ€™s key as the tableâ€™s primary key.\\n* `record_value`: Specifies that the primary key columns is sourced from the eventâ€™s value. You can set the `primaryKeyFields` property to define the primary key as a subset of fields from the eventâ€™s value; otherwise all fields are used by default.\\n\",\n           \"type\": \"string\"\n          },\n          \"quoteIdentifiers\": {\n           \"description\": \"Default `true`. Specifies whether generated SQL statements use quotation marks to delimit table and column names. See the Quoting and case sensitivity section for more details.\\n\",\n           \"type\": \"boolean\"\n          },\n          \"schemaEvolution\": {\n           \"description\": \"Default `basic`. Specifies how the connector evolves the destination table schemas. For more information, see Schema evolution. The following options are available:\\n`none`: Specifies that the connector does not evolve the destination schema.\\n`basic`: Specifies that basic evolution occurs. The connector adds missing columns to the table by comparing the incoming eventâ€™s record schema to the database table structure.\\n\",\n           \"type\": \"string\"\n          },\n          \"tableNameFormat\": {\n           \"description\": \"Default `${original}`. Specifies a string that determines how the destination table name is formatted, based on the topic name of the event. The placeholder ${original} is replaced with the schema name and the table name separated by a point character (`.`).\\n\",\n           \"type\": \"string\"\n          },\n          \"tableNamingStrategy\": {\n           \"description\": \"Default `io.stackgres.stream.jobs.migration.StreamMigrationTableNamingStrategy`. Specifies the fully-qualified class name of a TableNamingStrategy implementation that the connector uses to resolve table names from incoming event topic names.\\nThe default behavior is to:\\n* Replace the ${topic} placeholder in the `tableNameFormat` configuration property with the eventâ€™s topic.\\n* Sanitize the table name by replacing dots (`.`) with underscores (`_`).\\n\",\n           \"type\": \"string\"\n          },\n          \"truncateEnabled\": {\n           \"description\": \"Default `true`. Specifies whether the connector processes TRUNCATE events and truncates the corresponding tables from the database.\\nAlthough support for TRUNCATE statements has been available in Db2 since version 9.7, currently, the JDBC connector is unable to process standard TRUNCATE events that the Db2 connector emits.\\nTo ensure that the JDBC connector can process TRUNCATE events received from Db2, perform the truncation by using an alternative to the standard TRUNCATE TABLE statement. For example:\\n\\n```\\nALTER TABLE \\u003ctable_name\\u003e ACTIVATE NOT LOGGED INITIALLY WITH EMPTY TABLE\\n```\\n\\nThe user account that submits the preceding query requires ALTER privileges on the table to be truncated.\\n\",\n           \"type\": \"boolean\"\n          }\n         },\n         \"type\": \"object\"\n        },\n        \"name\": {\n         \"description\": \"The target SGCluster name.\\n\",\n         \"type\": \"string\"\n        },\n        \"password\": {\n         \"description\": \"The password used by the CDC sink process to connect to the database.\\n\\nIf not specified the default superuser password will be used.\\n\",\n         \"properties\": {\n          \"key\": {\n           \"description\": \"The Secret key where the password is stored.\\n\",\n           \"type\": \"string\"\n          },\n          \"name\": {\n           \"description\": \"The Secret name where the password is stored.\\n\",\n           \"type\": \"string\"\n          }\n         },\n         \"required\": [\n          \"name\",\n          \"key\"\n         ],\n         \"type\": \"object\"\n        },\n        \"skipDdlImport\": {\n         \"description\": \"When `true` disable import of DDL and tables will be created on demand by Debezium.\\n\",\n         \"type\": \"boolean\"\n        },\n        \"username\": {\n         \"description\": \"The username used by the CDC sink process to connect to the database.\\n\\nIf not specified the default superuser username (by default postgres) will be used.\\n\",\n         \"properties\": {\n          \"key\": {\n           \"description\": \"The Secret key where the username is stored.\\n\",\n           \"type\": \"string\"\n          },\n          \"name\": {\n           \"description\": \"The Secret name where the username is stored.\\n\",\n           \"type\": \"string\"\n          }\n         },\n         \"required\": [\n          \"name\",\n          \"key\"\n         ],\n         \"type\": \"object\"\n        }\n       },\n       \"required\": [\n        \"name\"\n       ],\n       \"type\": \"object\"\n      },\n      \"type\": {\n       \"description\": \"Indicate the type of target of this stream. Possible values are:\\n\\n* `CloudEvent`: events will be sent to a cloud event receiver.\\n* `PgLambda`: events will trigger the execution of a lambda script by integrating with [Knative Service](https://knative.dev/docs/serving/) (Knative must be already installed).\\n* `SGCluster`: events will be sinked to an SGCluster allowing migration of data.\\n\",\n       \"type\": \"string\"\n      }\n     },\n     \"required\": [\n      \"type\"\n     ],\n     \"type\": \"object\"\n    }\n   },\n   \"required\": [\n    \"source\",\n    \"target\"\n   ],\n   \"type\": \"object\"\n  }\n },\n \"required\": [\n  \"metadata\",\n  \"spec\"\n ],\n \"title\": \"SG Stream\",\n \"type\": \"object\"\n}",
   "version": "stackgres.io/v1alpha1"
  },
  "configuration": null,
  "description": "",
  "displayName": "SG Stream",
  "format": "JSON",
  "id": "00000000-0000-0000-0000-000000000000",
  "metadata": {
   "genealogy": "",
   "isAnnotation": false,
   "isNamespaced": true,
   "published": false,
   "source_uri": "https://stackgres.io/downloads/stackgres-k8s/stackgres/1.13.0/helm/stackgres-operator.tgz"
  },
  "model": {
   "category": {
    "name": "App Definition and Development"
   },
   "displayName": "Stackgres Operator",
   "id": "00000000-0000-0000-0000-000000000000",
   "metadata": {
    "isAnnotation": false,
    "primaryColor": "#426d88",
    "secondaryColor": "#00D3A9",
    "shape": "circle",
    "source_uri": "https://stackgres.io/downloads/stackgres-k8s/stackgres/1.13.0/helm/stackgres-operator.tgz",
    "styleOverrides": "",
    "svgColor": "\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e\u003c!DOCTYPE svg\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" id=\"Capa_2\" viewBox=\"0 0 500 500\" height=\"20\" width=\"20\"\u003e\u003cstyle xmlns=\"http://www.w3.org/2000/svg\"\u003e.st0{fill:#42a8c8}.st1{fill:#426d88}.st2{fill:#428bb4}.st3{fill:#16657c}.st4{fill:#39b54a}.st5{fill:#009245}.st6{fill:#f2c63f}.st7{fill:#f2b136}.st8{fill:#f2a130}.st9{fill:#ff7124}.st10{fill:#d93d1b}\u003c/style\u003e\u003cswitch xmlns=\"http://www.w3.org/2000/svg\"\u003e\u003cg xmlns=\"http://www.w3.org/2000/svg\"\u003e\u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st0\" d=\"M5.2 273.7l110.7 126.8h268.2l110.7-126.8-110.7-126.8H115.9z\"\u003e\u003c/path\u003e\u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st1\" d=\"M115.9 400.5h268.4v56.4H115.9z\"\u003e\u003c/path\u003e\u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st2\" d=\"M115.9 456.8L5.2 330v-56.3l110.7 126.8z\"\u003e\u003c/path\u003e\u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st3\" d=\"M384.1 456.8L494.8 330v-56.3L384.1 400.5z\"\u003e\u003c/path\u003e\u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st4\" d=\"M465.7 253.4c0-47-96.6-85.2-215.7-85.2S34.4 206.3 34.4 253.4s96.5 85.2 215.6 85.2 215.7-38.2 215.7-85.2z\"\u003e\u003c/path\u003e\u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st5\" d=\"M465.7 253.4c0 47-96.6 85.2-215.7 85.2S34.4 300.4 34.4 253.4v46.9c14.9 41.3 106.5 85.2 215.6 85.2s200.8-43.9 215.7-85.2v-46.9z\"\u003e\u003c/path\u003e\u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st6\" d=\"M16.3 178.6L250 314.1l233.8-135.5L250 43.2z\"\u003e\u003c/path\u003e\u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st7\" d=\"M16.3 178.6v52.8L250 366.9v-52.8z\"\u003e\u003c/path\u003e\u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st8\" d=\"M483.8 178.6L250 314.1v52.8l233.8-135.5z\"\u003e\u003c/path\u003e\u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st9\" d=\"M68 212.4l364.1-53-92.4-106z\"\u003e\u003c/path\u003e\u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st10\" d=\"M68 212.4l364.1-53v47.8L68 260.1z\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/switch\u003e\u003c/svg\u003e",
    "svgComplete": "",
    "svgWhite": "\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e\u003c!DOCTYPE svg\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\" id=\"Capa_2\" xmlns:_xmlns=\"xmlns\" _xmlns:xlink=\"http://www.w3.org/1999/xlink\" x=\"0px\" y=\"0px\" viewBox=\"0 0 500 500\" style=\"enable-background:new 0 0 500 500;\" xml:space=\"preserve\" height=\"20\" width=\"20\"\u003e\n\u003cstyle xmlns=\"http://www.w3.org/2000/svg\" type=\"text/css\"\u003e\n        .st0{fill:#FFFFFF;}\n        .st1{opacity:0.8;fill:#FFFFFF;}\n        .st2{opacity:0.9;fill:#FFFFFF;}\n\u003c/style\u003e\n\u003cg xmlns=\"http://www.w3.org/2000/svg\"\u003e\n        \u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st0\" d=\"M5.2,273.7l110.7,126.8h268.2l110.7-126.8L384.1,146.9H115.9L5.2,273.7z\"\u003e\u003c/path\u003e\n        \u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st1\" d=\"M115.9,400.5h268.4v56.4H115.9V400.5z\"\u003e\u003c/path\u003e\n        \u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st2\" d=\"M115.9,456.8L5.2,330v-56.3l110.7,126.8L115.9,456.8z\"\u003e\u003c/path\u003e\n        \u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st2\" d=\"M384.1,456.8L494.8,330v-56.3L384.1,400.5V456.8z\"\u003e\u003c/path\u003e\n        \u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st0\" d=\"M465.7,253.4c0-47-96.6-85.2-215.7-85.2S34.4,206.3,34.4,253.4s96.5,85.2,215.6,85.2S465.7,300.4,465.7,253.4\u0026#xA;                L465.7,253.4z\"\u003e\u003c/path\u003e\n        \u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st1\" d=\"M465.7,253.4c0,47-96.6,85.2-215.7,85.2S34.4,300.4,34.4,253.4v46.9c14.9,41.3,106.5,85.2,215.6,85.2\u0026#xA;                s200.8-43.9,215.7-85.2V253.4z\"\u003e\u003c/path\u003e\n        \u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st0\" d=\"M16.3,178.6L250,314.1l233.8-135.5L250,43.2L16.3,178.6z\"\u003e\u003c/path\u003e\n        \u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st2\" d=\"M16.3,178.6v52.8L250,366.9v-52.8L16.3,178.6z\"\u003e\u003c/path\u003e\n        \u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st1\" d=\"M483.8,178.6L250,314.1v52.8l233.8-135.5V178.6z\"\u003e\u003c/path\u003e\n        \u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st0\" d=\"M68,212.4l364.1-53l-92.4-106L68,212.4z\"\u003e\u003c/path\u003e\n        \u003cpath xmlns=\"http://www.w3.org/2000/svg\" class=\"st1\" d=\"M68,212.4l364.1-53v47.8L68,260.1V212.4z\"\u003e\u003c/path\u003e\n\u003c/g\u003e\n\u003c/svg\u003e"
   },
   "model": {
    "version": "1.13.0"
   },
   "name": "stackgres-operator",
   "registrant": {
    "created_at": "0001-01-01T00:00:00Z",
    "credential_id": "00000000-0000-0000-0000-000000000000",
    "deleted_at": "0001-01-01T00:00:00Z",
    "id": "00000000-0000-0000-0000-000000000000",
    "kind": "artifacthub",
    "name": "Artifact Hub",
    "status": "discovered",
    "sub_type": "",
    "type": "registry",
    "updated_at": "0001-01-01T00:00:00Z",
    "user_id": "00000000-0000-0000-0000-000000000000"
   },
   "connection_id": "00000000-0000-0000-0000-000000000000",
   "schemaVersion": "models.meshery.io/v1beta1",
   "status": "enabled",
   "subCategory": "Database",
   "version": "v1.0.0",
   "components": null,
   "relationships": null
  },
  "schemaVersion": "components.meshery.io/v1beta1",
  "status": "enabled",
  "styles": {
   "primaryColor": "#426d88",
   "secondaryColor": "#00D3A9",
   "shape": "circle",
   "svgColor": "\u003csvg id=\"Capa_2\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 500 500\"\u003e\u003cstyle\u003e.st0{fill:#42a8c8}.st1{fill:#426d88}.st2{fill:#428bb4}.st3{fill:#16657c}.st4{fill:#39b54a}.st5{fill:#009245}.st6{fill:#f2c63f}.st7{fill:#f2b136}.st8{fill:#f2a130}.st9{fill:#ff7124}.st10{fill:#d93d1b}\u003c/style\u003e\u003cswitch\u003e\u003cg\u003e\u003cpath class=\"st0\" d=\"M5.2 273.7l110.7 126.8h268.2l110.7-126.8-110.7-126.8H115.9z\"\u003e\u003c/path\u003e\u003cpath class=\"st1\" d=\"M115.9 400.5h268.4v56.4H115.9z\"\u003e\u003c/path\u003e\u003cpath class=\"st2\" d=\"M115.9 456.8L5.2 330v-56.3l110.7 126.8z\"\u003e\u003c/path\u003e\u003cpath class=\"st3\" d=\"M384.1 456.8L494.8 330v-56.3L384.1 400.5z\"\u003e\u003c/path\u003e\u003cpath class=\"st4\" d=\"M465.7 253.4c0-47-96.6-85.2-215.7-85.2S34.4 206.3 34.4 253.4s96.5 85.2 215.6 85.2 215.7-38.2 215.7-85.2z\"\u003e\u003c/path\u003e\u003cpath class=\"st5\" d=\"M465.7 253.4c0 47-96.6 85.2-215.7 85.2S34.4 300.4 34.4 253.4v46.9c14.9 41.3 106.5 85.2 215.6 85.2s200.8-43.9 215.7-85.2v-46.9z\"\u003e\u003c/path\u003e\u003cpath class=\"st6\" d=\"M16.3 178.6L250 314.1l233.8-135.5L250 43.2z\"\u003e\u003c/path\u003e\u003cpath class=\"st7\" d=\"M16.3 178.6v52.8L250 366.9v-52.8z\"\u003e\u003c/path\u003e\u003cpath class=\"st8\" d=\"M483.8 178.6L250 314.1v52.8l233.8-135.5z\"\u003e\u003c/path\u003e\u003cpath class=\"st9\" d=\"M68 212.4l364.1-53-92.4-106z\"\u003e\u003c/path\u003e\u003cpath class=\"st10\" d=\"M68 212.4l364.1-53v47.8L68 260.1z\"\u003e\u003c/path\u003e\u003c/g\u003e\u003c/switch\u003e\u003c/svg\u003e",
   "svgWhite": "\u003csvg version=\"1.1\" id=\"Capa_2\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" x=\"0px\" y=\"0px\"\n         viewBox=\"0 0 500 500\" style=\"enable-background:new 0 0 500 500;\" xml:space=\"preserve\"\u003e\n\u003cstyle type=\"text/css\"\u003e\n        .st0{fill:#FFFFFF;}\n        .st1{opacity:0.8;fill:#FFFFFF;}\n        .st2{opacity:0.9;fill:#FFFFFF;}\n\u003c/style\u003e\n\u003cg\u003e\n        \u003cpath class=\"st0\" d=\"M5.2,273.7l110.7,126.8h268.2l110.7-126.8L384.1,146.9H115.9L5.2,273.7z\"/\u003e\n        \u003cpath class=\"st1\" d=\"M115.9,400.5h268.4v56.4H115.9V400.5z\"/\u003e\n        \u003cpath class=\"st2\" d=\"M115.9,456.8L5.2,330v-56.3l110.7,126.8L115.9,456.8z\"/\u003e\n        \u003cpath class=\"st2\" d=\"M384.1,456.8L494.8,330v-56.3L384.1,400.5V456.8z\"/\u003e\n        \u003cpath class=\"st0\" d=\"M465.7,253.4c0-47-96.6-85.2-215.7-85.2S34.4,206.3,34.4,253.4s96.5,85.2,215.6,85.2S465.7,300.4,465.7,253.4\n                L465.7,253.4z\"/\u003e\n        \u003cpath class=\"st1\" d=\"M465.7,253.4c0,47-96.6,85.2-215.7,85.2S34.4,300.4,34.4,253.4v46.9c14.9,41.3,106.5,85.2,215.6,85.2\n                s200.8-43.9,215.7-85.2V253.4z\"/\u003e\n        \u003cpath class=\"st0\" d=\"M16.3,178.6L250,314.1l233.8-135.5L250,43.2L16.3,178.6z\"/\u003e\n        \u003cpath class=\"st2\" d=\"M16.3,178.6v52.8L250,366.9v-52.8L16.3,178.6z\"/\u003e\n        \u003cpath class=\"st1\" d=\"M483.8,178.6L250,314.1v52.8l233.8-135.5V178.6z\"/\u003e\n        \u003cpath class=\"st0\" d=\"M68,212.4l364.1-53l-92.4-106L68,212.4z\"/\u003e\n        \u003cpath class=\"st1\" d=\"M68,212.4l364.1-53v47.8L68,260.1V212.4z\"/\u003e\n\u003c/g\u003e\n\u003c/svg\u003e"
  },
  "version": "v1.0.0"
 }